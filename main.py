#!/usr/bin/env python3
"""
ğŸ‡®ğŸ‡· ×‘×•×˜ ××•×“×™×¢×™×Ÿ ×©×•×•×§×™ ×”×™××•×¨×™× â€” ××™×¨××Ÿ
=========================================
×× ×˜×¨ ×©×•×•×§×™ ×”×™××•×¨×™× (Polymarket + Kalshi) ×”×§×©×•×¨×™× ×œ××™×¨××Ÿ ×‘×œ×‘×“.
× ×™×ª×•×— AI ×¢× Claude ×œ×›×œ ×”×ª×¨××”. ×¢×‘×¨×™×ª ×‘×œ×‘×“.

×××¤×™×™× ×™×:
  - ××¢×§×‘ ××—×™×¨×™× ×›×œ 2-3 ×“×§×•×ª
  - ×”×ª×¨××•×ª ××¨×‘×™×˜×¨××–' (×¤×¢×¨ >5% ×‘×™×Ÿ ×¤×œ×˜×¤×•×¨××•×ª)
  - ×–×™×”×•×™ ×§×•×¨×œ×¦×™×•×ª ×—×¨×™×’×•×ª ×‘×™×Ÿ ×©×•×•×§×™× ×§×©×•×¨×™×
  - ××¢×§×‘ ×—×“×©×•×ª ××™×¨××Ÿ ×›×œ 5-10 ×“×§×•×ª
  - × ×™×ª×•×— AI ××¢××™×§ ×œ×›×œ ×”×ª×¨××”
"""

import asyncio
import json
import logging
import os
import re
import signal
import sys
from datetime import datetime, timezone, timedelta
from difflib import SequenceMatcher
from typing import Any, Optional

import aiohttp
import feedparser
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger
from dotenv import load_dotenv
from telegram import Bot
from telegram.constants import ParseMode
from telegram.error import TelegramError, RetryAfter

load_dotenv()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN", "")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID", "")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY", "")
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
STATE_FILE = os.getenv("STATE_FILE", "/data/bot_state.json")

# Scan intervals
MARKET_SCAN_MINUTES = 3       # Scan markets every 3 minutes
NEWS_SCAN_MINUTES = 7         # Scan news every 7 minutes

# Alert thresholds
ARBITRAGE_THRESHOLD_PCT = 5.0   # Alert if gap > 5% between platforms
BIG_MOVE_THRESHOLD_PCT = 10.0   # Alert if market moves > 10% in 24h
CORRELATION_MOVE_PCT = 10.0     # Correlation alert: one moves 10%+, other doesn't

# APIs
POLYMARKET_API = "https://gamma-api.polymarket.com"
KALSHI_API = "https://api.elections.kalshi.com/trade-api/v2"
CLAUDE_API = "https://api.anthropic.com/v1/messages"
CLAUDE_MODEL = "claude-sonnet-4-20250514"

# â”€â”€ Iran Keywords for Market Filtering â”€â”€
IRAN_KEYWORDS = [
    "iran", "iranian", "khamenei", "mojtaba", "supreme leader",
    "irgc", "revolutionary guard", "tehran", "persian",
    "assembly of experts", "ayatollah", "raisi",
    "iran nuclear", "iran sanction", "iran regime",
    "iran war", "iran strike", "iran attack",
    "iran deal", "jcpoa", "iran israel",
    "iran leadership", "iran succession",
    "iran collapse", "iran revolution",
]

# â”€â”€ Iran News Search Keywords (for RSS/Google News) â”€â”€
IRAN_NEWS_QUERIES = [
    "Iran Supreme Leader successor",
    "Mojtaba Khamenei",
    "Assembly of Experts Iran",
    "Iran leadership transition",
    "Iran regime change",
    "IRGC Iran",
    "Iran nuclear deal",
    "Iran Israel conflict",
    "Iran sanctions",
    "Khamenei health",
]

# â”€â”€ Known Correlated Market Pairs (Claude will also detect dynamically) â”€â”€
CORRELATION_HINTS = [
    ("supreme leader", "regime"),
    ("supreme leader", "succession"),
    ("nuclear", "sanctions"),
    ("war", "strike"),
    ("israel", "attack"),
    ("regime", "revolution"),
    ("irgc", "regime"),
    ("mojtaba", "supreme leader"),
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOGGING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

logging.basicConfig(
    level=getattr(logging, LOG_LEVEL, logging.INFO),
    format="%(asctime)s â”‚ %(levelname)-7s â”‚ %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger("iran-bot")
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("telegram").setLevel(logging.WARNING)
logging.getLogger("apscheduler").setLevel(logging.WARNING)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLAUDE AI ENGINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AI_SYSTEM = """××ª×” ×× ×œ×™×¡×˜ ××•×“×™×¢×™×Ÿ ×‘×›×™×¨ ×”××ª××—×” ×‘×©×•×•×§×™ ×”×™××•×¨×™× ×¤×•×œ×™×˜×™×™×, ×¢× ××•××—×™×•×ª ××™×•×—×“×ª ×‘××™×¨××Ÿ.

×”×™×“×¢ ×©×œ×š ×›×•×œ×œ:
- ×”××‘× ×” ×”×¤×•×œ×™×˜×™ ×©×œ ××™×¨××Ÿ (×× ×”×™×’ ×¢×œ×™×•×Ÿ, ××•×¢×¦×ª ×”××•××—×™×, ××©××¨×•×ª ×”××”×¤×›×”)
- ×©×—×§× ×™ ×”××¤×ª×— (×—××™× ××™, ××•×’'×ª×‘× ×—××™× ××™, ×¨××™×¡×™, IRGC)
- ×”×’×¨×¢×™×Ÿ ×”××™×¨×× ×™, ×¡× ×§×¦×™×•×ª, ×•-JCPOA
- ×”×“×™× ××™×§×” ×”××–×•×¨×™×ª (××™×¨××Ÿ-×™×©×¨××œ, ××™×¨××Ÿ-××¨×”"×‘)
- ×©×•×§×™ ×”×™××•×¨×™× (Polymarket, Kalshi) ×•××™×š ×œ×–×”×•×ª ×”×–×“×× ×•×™×•×ª

×›×œ×œ×™× ×§×¨×™×˜×™×™×:
1. ×›×ª×•×‘ ×ª××™×“ ×‘×¢×‘×¨×™×ª
2. ×”×™×” ××“×•×™×§ ×•××‘×•×¡×¡ ×¢×•×‘×“×•×ª â€” ××œ ×ª××¦×™× ×¡×™×‘×•×ª
3. ×¦×™×™×Ÿ ×ª××™×“ ×¨××ª ×‘×™×˜×—×•×Ÿ
4. ××œ ×ª×¤×—×“ ×œ×”×’×™×“ "×œ× ×‘×¨×•×¨" ×›×©××™×Ÿ ××¡×¤×™×§ ××™×“×¢
5. ×¢× ×” ×ª××™×“ ×‘-JSON ×‘×œ×‘×“, ×‘×œ×™ backticks

×›×œ×œ×™× ×œ×’×‘×™ ×ª××¨×™×›×™× ×•×ª×–××•×Ÿ:
6. ×©×•×•×§×™× ×©×”×ª××¨×™×š ×©×œ×”× ×›×‘×¨ ×¢×‘×¨ ×”× ×œ× ×¨×œ×•×•× ×˜×™×™× â€” ××œ ×ª× ×ª×— ××•×ª×, ××œ ×ª×—×–×” ×œ×”× ×ª× ×•×¢×•×ª
7. ×›×©××ª×” ×—×•×–×” ×ª×–××•×Ÿ, ×”×™×” ×¡×¤×¦×™×¤×™ ×•×œ×•×’×™ â€” ××œ ×ª×’×™×“ "12 ×©×¢×•×ª" ×¡×ª×. ×”×¡×‘×¨ ×œ××” ×“×•×•×§× ×˜×•×•×— ×”×–××Ÿ ×”×–×”
8. ×× ××™×¨×•×¢ ×§×¨×” ×œ×¤× ×™ X ×©×¢×•×ª ×•×œ× ×”×™×” ×©×™× ×•×™ ×¢×“ ×¢×›×©×™×•, ××œ ×ª×—×–×” ×©×™× ×•×™ ××™×™×“×™ ××œ× ×× ×™×© ×¡×™×‘×” ×—×“×©×” ×¡×¤×¦×™×¤×™×ª
9. ×œ×’×‘×™ ×ª×—×–×™×•×ª: ××” ×”××™×¨×•×¢ ×”×‘× ×©×¢×©×•×™ ×œ×’×¨×•× ×œ×ª× ×•×¢×”? (×œ××©×œ: ×”×›×¨×–×” ×¨×©××™×ª, ×¤×’×™×©×”, ×”×œ×•×•×™×”, ××™× ×•×™)"""


def get_current_datetime_str():
    """Get current date/time string for AI context."""
    now = datetime.now(timezone.utc)
    return now.strftime("%Y-%m-%d %H:%M UTC")


async def ask_claude(session: aiohttp.ClientSession, prompt: str, max_tokens: int = 1500) -> Optional[str]:
    """Call Claude API with current date context."""
    if not ANTHROPIC_API_KEY:
        logger.warning("No ANTHROPIC_API_KEY set")
        return None

    date_context = f"\n\n[×ª××¨×™×š ×•×©×¢×” × ×•×›×—×™×™×: {get_current_datetime_str()}]\n\n"

    headers = {
        "x-api-key": ANTHROPIC_API_KEY,
        "anthropic-version": "2023-06-01",
        "content-type": "application/json",
    }
    payload = {
        "model": CLAUDE_MODEL,
        "max_tokens": max_tokens,
        "system": AI_SYSTEM,
        "messages": [{"role": "user", "content": date_context + prompt}],
    }

    try:
        async with session.post(CLAUDE_API, json=payload, headers=headers,
                                timeout=aiohttp.ClientTimeout(total=45)) as r:
            if r.status != 200:
                err = await r.text()
                logger.error(f"Claude API {r.status}: {err[:200]}")
                return None
            data = await r.json()
            content = data.get("content", [])
            if content and content[0].get("type") == "text":
                return content[0]["text"]
    except Exception as e:
        logger.error(f"Claude API error: {e}")
    return None


def parse_claude_json(text: str) -> dict:
    """Safely parse Claude's JSON response."""
    if not text:
        return {}
    try:
        clean = text.strip()
        if clean.startswith("```"):
            clean = re.sub(r"```json?|```", "", clean).strip()
        return json.loads(clean)
    except json.JSONDecodeError:
        logger.warning(f"JSON parse failed: {text[:150]}")
        return {}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AI ANALYSIS FUNCTIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def ai_analyze_arbitrage(session, market_name, poly_price, kalshi_price, gap_pct, poly_url, kalshi_url):
    """Deep AI analysis of an arbitrage opportunity."""
    prompt = f"""× ×ª×— ×”×–×“×× ×•×ª ××¨×‘×™×˜×¨××–' ×‘×©×•×§ ×”×™××•×¨×™× ×”×§×©×•×¨ ×œ××™×¨××Ÿ:

×©×•×§: {market_name}
××—×™×¨ Polymarket: {poly_price}% (×›×Ÿ)
××—×™×¨ Kalshi: {kalshi_price}% (×›×Ÿ)
×¤×¢×¨: {gap_pct:.1f}%

× ×ª×— ×•×¢× ×” ×‘-JSON:
{{
    "title_he": "×©× ×”×©×•×§ ×‘×¢×‘×¨×™×ª",
    "context": "×”×§×©×¨ ×¤×•×œ×™×˜×™/×’×™××•×¤×•×œ×™×˜×™ â€” ××” ×§×•×¨×” ×‘××™×¨××Ÿ ×©×¨×œ×•×•× ×˜×™ ×œ×©×•×§ ×”×–×” (3-4 ××©×¤×˜×™×)",
    "why_gap": "×œ××” ×›× ×¨××” ×§×™×™× ×”×¤×¢×¨ â€” ×”×× ×–×” ×—×•×¡×¨ × ×–×™×œ×•×ª, ××™×“×¢ ××¡×™××˜×¨×™, ××• ×”×‘×“×œ ×‘×‘×¡×™×¡ ×”××©×ª××©×™× (2-3 ××©×¤×˜×™×)",
    "risk_assessment": "×”×¢×¨×›×ª ×¡×™×›×•×Ÿ â€” ××” ×”×¡×™×›×•× ×™× ×‘× ×™×¡×™×•×Ÿ ×œ× ×¦×œ ××ª ×”×¤×¢×¨ (2-3 ××©×¤×˜×™×)",
    "opportunity": "×”×× ×–×• ×”×–×“×× ×•×ª ×××™×ª×™×ª ××• ××œ×›×•×“×ª, ×•×‘××™×–×• ×¨××ª ×‘×™×˜×—×•×Ÿ (2-3 ××©×¤×˜×™×)",
    "recommendation": "×”××œ×¦×” ×¡×¤×¦×™×¤×™×ª â€” ××” ×›×“××™ ×œ×¢×©×•×ª ×•××” ×œ×, ×¢× ×¡×™×™×’×™× (2-3 ××©×¤×˜×™×)",
    "watch_factors": ["×’×•×¨× 1 ×œ×¢×§×•×‘", "×’×•×¨× 2 ×œ×¢×§×•×‘", "×’×•×¨× 3 ×œ×¢×§×•×‘"],
    "confidence": "×’×‘×•×”×”" ××• "×‘×™× ×•× ×™×ª" ××• "× ××•×›×”"
}}"""
    return parse_claude_json(await ask_claude(session, prompt))


async def ai_analyze_correlation(session, market_a, market_b, price_a, price_b, move_a, move_b):
    """Deep AI analysis of a correlation anomaly."""
    prompt = f"""×–×•×”×ª×” ×× ×•××œ×™×™×ª ×§×•×¨×œ×¦×™×” ×‘×™×Ÿ ×©× ×™ ×©×•×•×§×™ ×”×™××•×¨×™× ×”×§×©×•×¨×™× ×œ××™×¨××Ÿ:

×©×•×§ ×': {market_a['title']} â€” ××—×™×¨: {price_a}% â€” ×ª× ×•×¢×” 24 ×©×¢×•×ª: {move_a:+.1f}%
×©×•×§ ×‘': {market_b['title']} â€” ××—×™×¨: {price_b}% â€” ×ª× ×•×¢×” 24 ×©×¢×•×ª: {move_b:+.1f}%

×©×•×§ ××—×“ ×–×– ××©××¢×•×ª×™×ª ×‘×œ×™ ×©×”×©× ×™ ×”×’×™×‘ â€” ××” ×–×” ××•××¨?

×¢× ×” ×‘-JSON:
{{
    "title_he": "×›×•×ª×¨×ª ×§×¦×¨×” ×œ×× ×•××œ×™×” ×‘×¢×‘×¨×™×ª",
    "context": "×”×§×©×¨ â€” ×œ××” ×”×©×•×•×§×™× ×”××œ×” ×××•×¨×™× ×œ×”×™×•×ª ××§×•×©×¨×™× (2-3 ××©×¤×˜×™×)",
    "anomaly_explanation": "×”×¡×‘×¨ â€” ×œ××” ×›× ×¨××” ×©×•×§ ××—×“ ×–×– ×•×”×©× ×™ ×œ× (3-4 ××©×¤×˜×™×)",
    "opportunity": "×”×× ×™×© ×›××Ÿ ×”×–×“×× ×•×ª â€” ×”×× ×”×©×•×§ ×©×œ× ×–×– '××¤×’×¨' ××• ×©×”×•× ×¦×•×“×§ (2-3 ××©×¤×˜×™×)",
    "risk_assessment": "×¡×™×›×•× ×™× â€” ××” ×™×›×•×œ ×œ×”×©×ª×‘×© ×× ×¤×•×¢×œ×™× ×¢×œ ×”×¤×¢×¨ (2 ××©×¤×˜×™×)",
    "recommendation": "×”××œ×¦×” ×¡×¤×¦×™×¤×™×ª ×¢× ×¡×™×™×’×™× (2-3 ××©×¤×˜×™×)",
    "expected_resolution": "××” ×¦×¤×•×™ ×œ×§×¨×•×ª â€” ×”×× ×”×¤×¢×¨ ×™×™×¡×’×¨ ×•××™×š (2 ××©×¤×˜×™×)",
    "watch_factors": ["×’×•×¨× 1", "×’×•×¨× 2", "×’×•×¨× 3"],
    "confidence": "×’×‘×•×”×”" ××• "×‘×™× ×•× ×™×ª" ××• "× ××•×›×”"
}}"""
    return parse_claude_json(await ask_claude(session, prompt))


async def ai_analyze_big_move(session, market, old_price, new_price, timeframe, recent_news=None):
    """Analyze a big price move using REAL recent news â€” not guessing."""
    direction = "×¢×œ×™×™×”" if new_price > old_price else "×™×¨×™×“×”"

    news_context = ""
    if recent_news:
        news_lines = "\n".join([
            f"  - [{n['source']}] {n['title']}"
            for n in recent_news[:8]
        ])
        news_context = f"""
â•â• ×—×“×©×•×ª ××—×¨×•× ×•×ª ×©× ××¦××• (×”×©×ª××© ×‘×”×Ÿ!) â•â•
{news_lines}
"""
    else:
        news_context = "\nâ•â• ×œ× × ××¦××• ×—×“×©×•×ª ×¨×œ×•×•× ×˜×™×•×ª â•â•\n"

    prompt = f"""×©×•×§ ×”×™××•×¨×™× ×”×§×©×•×¨ ×œ××™×¨××Ÿ ×–×– ×‘×¦×•×¨×” ××©××¢×•×ª×™×ª:

×©×•×§: {market['title']}
××—×™×¨ ×§×•×“×: {old_price}%
××—×™×¨ × ×•×›×—×™: {new_price}%
×©×™× ×•×™: {new_price - old_price:+.1f}%
×›×™×•×•×Ÿ: {direction}
×˜×•×•×— ×–××Ÿ: {timeframe}
{news_context}

×”×•×¨××•×ª ×§×¨×™×˜×™×•×ª:
1. ×× ×™×© ×—×“×©×•×ª ×¨×œ×•×•× ×˜×™×•×ª â€” ×”×©×ª××© ×‘×”×Ÿ ×›×“×™ ×œ×”×¡×‘×™×¨ ××ª ×”×ª× ×•×¢×”. ××œ ×ª× ×—×©!
2. ×× ××™×Ÿ ×—×“×©×•×ª â€” ×××•×¨ ×‘×¤×™×¨×•×© ×©×œ× ×‘×¨×•×¨ ××” ×’×¨× ×œ×ª× ×•×¢×”
3. ××œ ×ª××¦×™× ×¡×™×‘×•×ª. ×× ××ª×” ×œ× ×™×•×“×¢ â€” ×××•×¨ "×œ× ×‘×¨×•×¨"
4. ×”×ª××§×“ ×‘×ª×—×–×™×ª ×§×“×™××”: ××” ×¦×¤×•×™ ×œ×§×¨×•×ª ×¢×›×©×™×•?

×¢× ×” ×‘-JSON:
{{
    "title_he": "×©× ×”×©×•×§ ×‘×¢×‘×¨×™×ª",
    "cause": "××” ×’×¨× ×œ×ª× ×•×¢×” â€” ×¢×œ ×¡××š ×—×“×©×•×ª ×××™×ª×™×•×ª ×‘×œ×‘×“. ×× ××™×Ÿ ×—×“×©×•×ª ×¨×œ×•×•× ×˜×™×•×ª ×›×ª×•×‘ '×”×¡×™×‘×” ×œ× ×‘×¨×•×¨×” ×›×¨×’×¢' (2-3 ××©×¤×˜×™×)",
    "news_based": true ××• false,
    "forward_prediction": "×ª×—×–×™×ª ×§×“×™××”: ××” ×¦×¤×•×™ ×œ×§×¨×•×ª ×‘×©×•×§ ×”×–×” ×‘×™××™× ×”×§×¨×•×‘×™×, ×¢×œ ×¡××š ×”×—×“×©×•×ª ×•×”××’××” (3-4 ××©×¤×˜×™×)",
    "related_markets_prediction": "××™×œ×• ×©×•×•×§×™× ××—×¨×™× ×¦×¤×•×™×™× ×œ×–×•×– ×‘×¢×§×‘×•×ª ×–×”, ×•×œ××™×–×” ×›×™×•×•×Ÿ (2-3 ××©×¤×˜×™×)",
    "action_window": "×—×œ×•×Ÿ ×”×¤×¢×•×œ×”: ×›××” ×–××Ÿ ×œ×“×¢×ª×š ×”×”×–×“×× ×•×ª ×¤×ª×•×—×” (××©×¤×˜ ××—×“)",
    "recommendation": "×”××œ×¦×” ×¡×¤×¦×™×¤×™×ª â€” ××” ×œ×¢×©×•×ª ×¢×›×©×™×• (2-3 ××©×¤×˜×™×)",
    "watch_factors": ["×’×•×¨× 1 ×œ×¢×§×•×‘ ×©×™×©×¤×™×¢ ×¢×œ ×”×›×™×•×•×Ÿ", "×’×•×¨× 2", "×’×•×¨× 3"],
    "confidence": "×’×‘×•×”×”" ××• "×‘×™× ×•× ×™×ª" ××• "× ××•×›×”"
}}"""
    return parse_claude_json(await ask_claude(session, prompt))


async def ai_filter_news(session, news_items, knowledge_base, sent_topics):
    """AI decides which news items are truly NEW and worth sending."""
    news_text = "\n".join([
        f"  {i+1}. [{item['source']}] {item['title']}"
        for i, item in enumerate(news_items[:10])
    ])

    topics_text = "\n".join([f"  - {t}" for t in sent_topics[-20:]]) if sent_topics else "  (××™×Ÿ × ×•×©××™× ×§×•×“××™×)"

    prompt = f"""××ª×” ××¡× ×Ÿ ×—×“×©×•×ª ×—×›×. ×”×ª×¤×§×™×“ ×©×œ×š: ×œ×–×”×•×ª ××™×œ×• ×—×“×©×•×ª ×”×Ÿ ×‘×××ª ×—×“×©×•×ª ×•××™×œ×• ×”×Ÿ ×—×–×¨×” ×¢×œ ××™×“×¢ ×™×©×Ÿ.

â•â• ××” ×©×× ×—× ×• ×›×‘×¨ ×™×•×“×¢×™× (××¦×‘ ×¢×“×›× ×™) â•â•
{knowledge_base or "(××™×Ÿ ××™×“×¢ ×§×•×“× â€” ×–×• ×”×¡×¨×™×§×” ×”×¨××©×•× ×”)"}

â•â• × ×•×©××™× ×©×›×‘×¨ ×“×™×•×•×—× ×• ×¢×œ×™×”× â•â•
{topics_text}

â•â• ×—×“×©×•×ª ×©×”×ª×§×‘×œ×• ×¢×›×©×™×• â•â•
{news_text}

×‘×“×•×§ ×›×œ ×—×“×©×” ×•×©××œ ××ª ×¢×¦××š:
1. ×”×× ×–×” ××™×“×¢ ×©×›×‘×¨ ×™×“×•×¢ ×œ× ×• ××”××¦×‘ ×”×¢×“×›× ×™?
2. ×”×× ×–×” ×—×•×–×¨ ×¢×œ × ×•×©× ×©×›×‘×¨ ×“×™×•×•×—× ×• ×¢×œ×™×•?
3. ×”×× ×™×© ×›××Ÿ ×¤×¨×˜ ×—×“×© ××©××¢×•×ª×™ ×©×œ× ×™×“×¢× ×•?

×œ×“×•×’××”:
- ×× ×›×‘×¨ ×™×“×•×¢ ×©×—××™× ××™ ××ª â†’ ×›×ª×‘×” "×—××™× ××™ ××ª" = ×œ× ×—×“×©, ×œ×¡× ×Ÿ
- ×× ×›×‘×¨ ×™×“×•×¢ ×©×—××™× ××™ ××ª â†’ ×›×ª×‘×” "××•×’'×ª×‘× ××•× ×” ×›×××œ× ××§×•×" = ×—×“×©! ×œ×©×œ×•×—
- ×× ×›×‘×¨ ×“×™×•×•×—× ×• ×¢×œ ×¡× ×§×¦×™×•×ª ×—×“×©×•×ª â†’ ×›×ª×‘×” × ×•×¡×¤×ª ×¢×œ ××•×ª×Ÿ ×¡× ×§×¦×™×•×ª = ×œ× ×—×“×©

×¢× ×” ×‘-JSON ×‘×œ×‘×“:
{{
    "selected_indices": [1, 4],
    "reasoning": "×”×¡×‘×¨ ×§×¦×¨ ×œ××” ×‘×—×¨×ª ×¨×§ ××ª ××œ×” ×•×œ××” ×¡×™× × ×ª ××ª ×”××—×¨×™×"
}}

×× ××£ ×—×“×©×” ×œ× ××‘×™××” ××™×“×¢ ×—×“×©, ×”×—×–×¨: {{"selected_indices": [], "reasoning": "×”×¡×‘×¨"}}"""

    result = parse_claude_json(await ask_claude(session, prompt))
    if not result:
        return news_items[:3]  # Fallback: send first 3

    selected = result.get("selected_indices", [])
    reasoning = result.get("reasoning", "")
    if reasoning:
        logger.info(f"AI filter: {reasoning}")

    filtered = []
    for idx in selected:
        i = idx - 1  # Convert 1-indexed to 0-indexed
        if 0 <= i < len(news_items):
            filtered.append(news_items[i])

    return filtered


async def ai_analyze_news(session, news_items, current_markets, knowledge_base):
    """PREDICTIVE analysis: news â†’ forecast which markets WILL move and how."""
    markets_summary = "\n".join([
        f"  - {m['title']}: {m['yes_price']*100:.1f}% ({m['source']})"
        for m in current_markets[:15]
    ]) or "  ××™×Ÿ ×©×•×•×§×™× ×¤×¢×™×œ×™× ×›×¨×’×¢"

    news_text = "\n".join([
        f"  - [{item['source']}] {item['title']}"
        for item in news_items[:5]
    ])

    prompt = f"""××ª×” ×× ×œ×™×¡×˜ ×©×•×•×§×™ ×”×™××•×¨×™×. ×”×ª×¤×§×™×“ ×©×œ×š: ×œ×§×¨×•× ×—×“×©×•×ª ×•×œ×—×–×•×ª ××™×š ×©×•×•×§×™ ×”×”×™××•×¨×™× ×™×’×™×‘×• â€” ×œ×¤× ×™ ×©×–×” ×§×•×¨×”.

â•â• ××” ×©×× ×—× ×• ×›×‘×¨ ×™×•×“×¢×™× â•â•
{knowledge_base or "(××™×Ÿ ××™×“×¢ ×§×•×“×)"}

â•â• ×—×“×©×•×ª ×—×“×©×•×ª â•â•
{news_text}

â•â• ×©×•×•×§×™× ×¤×¢×™×œ×™× ×›×¨×’×¢ â•â•
{markets_summary}

×”×•×¨××•×ª ×§×¨×™×˜×™×•×ª:
1. ××œ ×ª×¡×›× ××ª ×”×—×“×©×•×ª â€” ×”××©×ª××© ×™×›×•×œ ×œ×§×¨×•× ×‘×¢×¦××•
2. ×”×ª××§×“ ×‘: ××” ×”×—×“×©×•×ª ×”××œ×” ××•××¨×•×ª ×¢×œ ×”×¢×ª×™×“ ×©×œ ×”×©×•×•×§×™×
3. ×—×–×” ×ª× ×•×¢×•×ª ×¡×¤×¦×™×¤×™×•×ª: ××™×–×” ×©×•×§, ×œ××™×–×” ×›×™×•×•×Ÿ, ×‘×›××”, ×•××ª×™
4. ×× ×—×“×©×” ××¦×‘×™×¢×” ×¢×œ ×”×–×“×× ×•×ª â€” ×¦×™×™×Ÿ ××•×ª×” ×‘×‘×™×¨×•×¨
5. ×”×™×” ×¡×¤×¦×™×¤×™: "×©×•×§ X ×¦×¤×•×™ ×œ×¢×œ×•×ª ×-60% ×œ-70-75% ×ª×•×š 24-48 ×©×¢×•×ª" â€” ×œ× "×™×™×ª×›×Ÿ ×©×™×”×™×” ×©×™× ×•×™"
6. ×‘×“×•×§ ××ª ×”×ª××¨×™×š ×”× ×•×›×—×™! ×× ×©×•×§ ××ª×™×™×—×¡ ×œ×ª××¨×™×š ×©×›×‘×¨ ×¢×‘×¨ (×œ××©×œ "×™× ×•××¨ 2026" ×›×©×× ×—× ×• ×‘××¨×¥ 2026) â€” ×”×ª×¢×œ× ××× ×• ×œ×—×œ×•×˜×™×Ÿ, ××œ ×ª×›×œ×•×œ ××•×ª×• ×‘×ª×—×–×™×•×ª
7. ×œ×’×‘×™ ×ª×–××•×Ÿ ×”×ª×—×–×™×•×ª â€” ×”×™×” ×œ×•×’×™:
   - ×¦×™×™×Ÿ ××” ×”××™×¨×•×¢ ×”×‘× ×©×™×’×¨×•× ×œ×ª× ×•×¢×” (×œ××©×œ: "×”×›×¨×–×” ×¨×©××™×ª ×¢×œ ×™×•×¨×©", "×”×œ×•×•×™×”", "×”×¦×‘×¢×” ×‘××•×¢×¦×ª ×”××•××—×™×")
   - ×× ××™×¨×•×¢ ×›×‘×¨ ×§×¨×” ×œ×¤× ×™ 20 ×©×¢×•×ª ×•×”×©×•×§ ×›×‘×¨ ×”×’×™×‘ â€” ××œ ×ª×—×–×” ×ª× ×•×¢×” × ×•×¡×¤×ª ××œ× ×× ×™×© ×˜×¨×™×’×¨ ×—×“×© ×¡×¤×¦×™×¤×™
   - ×”×˜×•×•×— ×¦×¨×™×š ×œ×”×™×•×ª ××‘×•×¡×¡ ×¢×œ ××™×¨×•×¢ ×¦×¤×•×™, ×œ× ××¡×¤×¨ ×©×¨×™×¨×•×ª×™

×¢× ×” ×‘-JSON:
{{
    "headline_he": "×›×•×ª×¨×ª ×§×¦×¨×” ×©××ª××§×“×ª ×‘×”×©×¤×¢×” ×¢×œ ×”×”×™××•×¨×™×, ×œ× ×‘×—×“×©×•×ª ×¢×¦××Ÿ (××©×¤×˜ ××—×“)",
    "news_summary_he": "×¡×™×›×•× ×§×¦×¨ ×‘×œ×‘×“ ×©×œ ×”×—×“×©×•×ª (2 ××©×¤×˜×™× ××§×¡×™××•×)",
    "predictions": [
        {{
            "market": "×©× ×”×©×•×§ ×©×¦×¤×•×™ ×œ×–×•×–",
            "current_price": "×”××—×™×¨ ×”× ×•×›×—×™",
            "predicted_price": "×”××—×™×¨ ×”×¦×¤×•×™",
            "direction": "×¢×œ×™×™×”" ××• "×™×¨×™×“×”",
            "trigger_event": "××” ×”××™×¨×•×¢ ×”×¡×¤×¦×™×¤×™ ×©×™×’×¨×•× ×œ×ª× ×•×¢×” (×œ××©×œ: '×”×›×¨×–×” ×¨×©××™×ª ×¢×œ ×™×•×¨×©', '×ª×•×¦××•×ª ×”×¦×‘×¢×”')",
            "timeframe": "××ª×™ ×¦×¤×•×™ ×”×˜×¨×™×’×¨ (×œ× ××¡×¤×¨ ×©×¨×™×¨×•×ª×™ â€” ××‘×•×¡×¡ ×¢×œ ×”××™×¨×•×¢)",
            "confidence": "×’×‘×•×”×”/×‘×™× ×•× ×™×ª/× ××•×›×”",
            "logic": "×œ××” â€” ×”×§×©×¨ ×™×©×™×¨ ×‘×™×Ÿ ×”×—×“×©×” ×œ×ª× ×•×¢×” ×”×¦×¤×•×™×” (××©×¤×˜ ××—×“)"
        }}
    ],
    "opportunity": "×”×”×–×“×× ×•×ª ×”××¨×›×–×™×ª: ××” ××¤×©×¨ ×œ×¢×©×•×ª ×¢×›×©×™×• ×œ×¤× ×™ ×©×”×©×•×§ ×™×’×™×‘ (2-3 ××©×¤×˜×™×)",
    "risk_warning": "×¡×™×›×•× ×™×: ××” ×™×›×•×œ ×œ×”×©×ª×‘×© ×¢× ×”×ª×—×–×™×ª (1-2 ××©×¤×˜×™×)",
    "action_items": ["×¤×¢×•×œ×” ×¡×¤×¦×™×¤×™×ª 1", "×¤×¢×•×œ×” ×¡×¤×¦×™×¤×™×ª 2"],
    "urgency": "×“×—×•×£" ××• "×—×©×•×‘" ××• "×œ×™×“×™×¢×”",
    "overall_confidence": "×’×‘×•×”×”" ××• "×‘×™× ×•× ×™×ª" ××• "× ××•×›×”",
    "topic_summary": "×ª×™××•×¨ ×§×¦×¨ ×©×œ ×”× ×•×©× (10-15 ××™×œ×™×)"
}}"""
    return parse_claude_json(await ask_claude(session, prompt, max_tokens=2000))


async def ai_update_knowledge(session, current_knowledge, new_info, news_titles):
    """AI updates the running knowledge base with new confirmed information."""
    news_list = "\n".join([f"  - {t}" for t in news_titles[:5]])

    prompt = f"""×¢×“×›×Ÿ ××ª ×××’×¨ ×”×™×“×¢ ×©×œ× ×• ×¢×œ ×”××¦×‘ ×‘××™×¨××Ÿ.

â•â• ×××’×¨ ×™×“×¢ × ×•×›×—×™ â•â•
{current_knowledge or "(×¨×™×§ â€” ×–×• ×”×”×ª×—×œ×”)"}

â•â• ××™×“×¢ ×—×“×© ×©×”×ª×§×‘×œ â•â•
{new_info}

â•â• ×›×•×ª×¨×•×ª ××§×•×¨ â•â•
{news_list}

×›×ª×•×‘ ×××’×¨ ×™×“×¢ ××¢×•×“×›×Ÿ. ×”×›×œ×œ×™×:
1. ×©××•×¨ ××ª ×›×œ ×”××™×“×¢ ×”×™×©×Ÿ ×©×¢×“×™×™×Ÿ ×¨×œ×•×•× ×˜×™
2. ×”×•×¡×£ ××ª ×”××™×“×¢ ×”×—×“×©
3. ×× ××™×“×¢ ×—×“×© ×¡×•×ª×¨ ××™×“×¢ ×™×©×Ÿ â€” ×¢×“×›×Ÿ (×œ××©×œ: ×× ×§×•×“× ×›×ª×‘× ×• "×—××™× ××™ ×—×•×œ×”" ×•×¢×›×©×™×• "×—××™× ××™ ××ª" â€” ×¢×“×›×Ÿ ×œ"××ª")
4. ×¡××Ÿ ×ª××¨×™×›×™× ×›×©××¤×©×¨
5. ×›×ª×•×‘ ×‘×¦×•×¨×” ×ª××¦×™×ª×™×ª â€” × ×§×•×“×•×ª ×§×¦×¨×•×ª
6. ××§×¡×™××•× 500 ××™×œ×™×

×¢× ×” ×‘-JSON:
{{
    "updated_knowledge": "×”×××’×¨ ×”××¢×•×“×›×Ÿ ×›×˜×§×¡×˜ ××•×‘× ×”"
}}"""

    result = parse_claude_json(await ask_claude(session, prompt, max_tokens=1500))
    return result.get("updated_knowledge", current_knowledge) if result else current_knowledge


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MARKET DATA FETCHING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def is_iran_market(title: str, description: str = "") -> bool:
    """Check if a market is related to Iran."""
    text = f"{title} {description}".lower()
    return any(kw in text for kw in IRAN_KEYWORDS)


def is_expired_market(title: str) -> bool:
    """Check if a market's date has already passed."""
    now = datetime.now(timezone.utc)
    text = title.lower()

    # Month names to numbers
    months = {
        "january": 1, "february": 2, "march": 3, "april": 4,
        "may": 5, "june": 6, "july": 7, "august": 8,
        "september": 9, "october": 10, "november": 11, "december": 12,
        "jan": 1, "feb": 2, "mar": 3, "apr": 4,
        "jun": 6, "jul": 7, "aug": 8, "sep": 9, "oct": 10, "nov": 11, "dec": 12,
    }

    # Pattern: "by/in/before January 2026" or "in January 2026"
    for month_name, month_num in months.items():
        if month_name in text:
            # Try to find a year nearby
            year_match = re.search(r'20(\d{2})', text)
            if year_match:
                year = 2000 + int(year_match.group(1))
                # If the end of that month is in the past
                if year < now.year or (year == now.year and month_num < now.month):
                    logger.debug(f"Expired market filtered: {title}")
                    return True

    return False


async def fetch_polymarket_iran(session: aiohttp.ClientSession) -> list[dict]:
    """Fetch Iran-related markets from Polymarket."""
    markets = []
    try:
        # Fetch a large batch and filter for Iran
        url = f"{POLYMARKET_API}/events"
        params = {"active": "true", "closed": "false", "limit": 200,
                  "order": "volume24hr", "ascending": "false"}
        async with session.get(url, params=params, timeout=aiohttp.ClientTimeout(total=30)) as r:
            if r.status != 200:
                logger.error(f"Polymarket API: {r.status}")
                return markets
            data = await r.json()

        for ev in data:
            title = ev.get("title", "")
            desc = ev.get("description", "")
            if not is_iran_market(title, desc) or is_expired_market(title):
                continue

            ev_markets = ev.get("markets", [])
            if not ev_markets:
                continue

            slug = ev.get("slug", "")
            ev_url = f"https://polymarket.com/event/{slug}" if slug else ""

            for m in ev_markets:
                outcomes = []
                op = m.get("outcomePrices", "")
                if op:
                    try:
                        prices = json.loads(op)
                        if len(prices) > 0:
                            outcomes.append(float(prices[0]))
                        if len(prices) > 1:
                            outcomes.append(float(prices[1]))
                    except (json.JSONDecodeError, IndexError, ValueError):
                        pass

                if not outcomes:
                    yp = m.get("bestAsk") or m.get("lastTradePrice")
                    if yp:
                        outcomes = [float(yp), 1.0 - float(yp)]

                if not outcomes:
                    continue

                m_title = m.get("question", m.get("groupItemTitle", title))
                if is_expired_market(m_title):
                    continue
                markets.append({
                    "id": f"poly_{m.get('id', ev.get('id', ''))}",
                    "title": m_title,
                    "event_title": title,
                    "yes_price": outcomes[0],
                    "source": "Polymarket",
                    "url": ev_url,
                    "volume": float(m.get("volume", 0) or 0),
                })

        # Also search specifically for Iran
        for query_term in ["iran", "khamenei", "supreme leader iran"]:
            try:
                search_url = f"{POLYMARKET_API}/events"
                search_params = {"active": "true", "closed": "false", "limit": 50,
                                 "tag": query_term}
                async with session.get(search_url, params=search_params,
                                       timeout=aiohttp.ClientTimeout(total=15)) as r:
                    if r.status != 200:
                        continue
                    search_data = await r.json()

                for ev in search_data:
                    title = ev.get("title", "")
                    ev_id = ev.get("id", "")
                    # Skip if already found
                    if any(f"poly_{ev_id}" in m["id"] for m in markets):
                        continue
                    if not is_iran_market(title, ev.get("description", "")) or is_expired_market(title):
                        continue

                    slug = ev.get("slug", "")
                    for m in ev.get("markets", []):
                        op = m.get("outcomePrices", "")
                        outcomes = []
                        if op:
                            try:
                                prices = json.loads(op)
                                if prices:
                                    outcomes = [float(prices[0])]
                            except Exception:
                                pass
                        if not outcomes:
                            yp = m.get("bestAsk") or m.get("lastTradePrice")
                            if yp:
                                outcomes = [float(yp)]
                        if not outcomes:
                            continue

                        m_title = m.get("question", m.get("groupItemTitle", title))
                        if is_expired_market(m_title):
                            continue
                        markets.append({
                            "id": f"poly_{m.get('id', ev_id)}",
                            "title": m_title,
                            "event_title": title,
                            "yes_price": outcomes[0],
                            "source": "Polymarket",
                            "url": f"https://polymarket.com/event/{slug}" if slug else "",
                            "volume": float(m.get("volume", 0) or 0),
                        })
            except Exception as e:
                logger.debug(f"Polymarket search '{query_term}': {e}")

        # Deduplicate by id
        seen = set()
        unique = []
        for m in markets:
            if m["id"] not in seen:
                seen.add(m["id"])
                unique.append(m)
        markets = unique

        logger.info(f"Polymarket Iran: {len(markets)} markets")
    except Exception as e:
        logger.error(f"Polymarket error: {e}")
    return markets


async def fetch_kalshi_iran(session: aiohttp.ClientSession) -> list[dict]:
    """Fetch Iran-related markets from Kalshi."""
    markets = []
    try:
        url = f"{KALSHI_API}/markets"
        params = {"limit": 500, "status": "open"}
        headers = {"Accept": "application/json"}
        async with session.get(url, params=params, headers=headers,
                               timeout=aiohttp.ClientTimeout(total=30)) as r:
            if r.status != 200:
                logger.warning(f"Kalshi API: {r.status}")
                return markets
            data = await r.json()

        for m in data.get("markets", []):
            title = m.get("title", "")
            subtitle = m.get("subtitle", "")
            if not is_iran_market(title, subtitle) or is_expired_market(title):
                continue

            yp = (m.get("yes_ask", 0) or m.get("last_price", 0) or 0) / 100.0
            ticker = m.get("ticker", "")
            markets.append({
                "id": f"kalshi_{m.get('id', '')}",
                "title": title,
                "event_title": title,
                "yes_price": yp,
                "source": "Kalshi",
                "url": f"https://kalshi.com/markets/{ticker.lower()}" if ticker else "",
                "volume": m.get("volume", 0) or 0,
                "subtitle": subtitle,
            })

        logger.info(f"Kalshi Iran: {len(markets)} markets")
    except Exception as e:
        logger.error(f"Kalshi error: {e}")
    return markets


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NEWS FETCHING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def fetch_iran_news(session: aiohttp.ClientSession) -> list[dict]:
    """Fetch Iran-related news from Google News RSS and other sources."""
    items = []

    # Google News RSS allows keyword search
    for query in IRAN_NEWS_QUERIES:
        encoded = query.replace(" ", "+")
        url = f"https://news.google.com/rss/search?q={encoded}&hl=en&gl=US&ceid=US:en"
        try:
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=15),
                                   headers={"User-Agent": "MarketIntelBot/1.0"}) as r:
                if r.status != 200:
                    continue
                content = await r.text()

            feed = feedparser.parse(content)
            for entry in feed.entries[:5]:
                title = entry.get("title", "")
                link = entry.get("link", "")
                guid = entry.get("id", entry.get("guid", link))

                # Extract source from Google News title format "Title - Source"
                source = "Google News"
                if " - " in title:
                    parts = title.rsplit(" - ", 1)
                    title = parts[0]
                    source = parts[1] if len(parts) > 1 else source

                pub = None
                if hasattr(entry, "published_parsed") and entry.published_parsed:
                    try:
                        pub = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                    except (TypeError, ValueError):
                        pass

                items.append({
                    "title": title,
                    "source": source,
                    "link": link,
                    "guid": guid,
                    "pub": pub,
                    "query": query,
                })
        except Exception as e:
            logger.debug(f"News fetch '{query}': {e}")

    # Deduplicate by title similarity
    unique = []
    seen_titles = []
    for item in items:
        is_dup = False
        for st in seen_titles:
            if SequenceMatcher(None, item["title"].lower(), st).ratio() > 0.8:
                is_dup = True
                break
        if not is_dup:
            unique.append(item)
            seen_titles.append(item["title"].lower())

    # Sort by date (newest first)
    unique.sort(key=lambda x: x.get("pub") or datetime.min.replace(tzinfo=timezone.utc), reverse=True)

    logger.info(f"Iran news: {len(unique)} unique items")
    return unique


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ANALYSIS ENGINES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def find_arbitrage_opportunities(poly_markets, kalshi_markets, threshold_pct=None):
    """Find price gaps between matched markets on different platforms."""
    if threshold_pct is None:
        threshold_pct = ARBITRAGE_THRESHOLD_PCT

    opportunities = []
    used_kalshi = set()

    for pm in poly_markets:
        best_match = None
        best_score = 0

        for km in kalshi_markets:
            if km["id"] in used_kalshi:
                continue

            # Compare titles
            pm_norm = pm["title"].lower().strip()
            km_norm = km["title"].lower().strip()
            score = SequenceMatcher(None, pm_norm, km_norm).ratio()

            # Also check event title vs subtitle
            if km.get("subtitle"):
                alt = SequenceMatcher(None, pm_norm, km["subtitle"].lower()).ratio()
                score = max(score, alt)
            if pm.get("event_title"):
                alt = SequenceMatcher(None, pm["event_title"].lower(), km_norm).ratio()
                score = max(score, alt)

            if score > best_score:
                best_score = score
                best_match = km

        if best_match and best_score >= 0.45:  # Lower threshold for Iran-specific
            used_kalshi.add(best_match["id"])
            poly_pct = pm["yes_price"] * 100
            kalshi_pct = best_match["yes_price"] * 100
            gap = abs(poly_pct - kalshi_pct)

            if gap >= threshold_pct:
                opportunities.append({
                    "name": pm["title"],
                    "poly_price": round(poly_pct, 1),
                    "kalshi_price": round(kalshi_pct, 1),
                    "gap_pct": round(gap, 1),
                    "poly_url": pm["url"],
                    "kalshi_url": best_match["url"],
                    "match_score": round(best_score, 2),
                })

    opportunities.sort(key=lambda x: x["gap_pct"], reverse=True)
    return opportunities


def find_correlation_anomalies(all_markets, price_history, threshold_pct=None):
    """Find markets that should be correlated but moved differently."""
    if threshold_pct is None:
        threshold_pct = CORRELATION_MOVE_PCT

    anomalies = []

    # Calculate 24h moves for each market
    moves = {}
    for m in all_markets:
        mid = m["id"]
        current = m["yes_price"] * 100
        history = price_history.get(mid, [])
        if not history:
            continue

        # Find price ~24h ago (or oldest available)
        oldest_price = history[0]["price"] * 100
        move = current - oldest_price
        moves[mid] = {"market": m, "current": current, "move": move}

    # Check correlation pairs
    market_list = list(moves.values())
    for i, a in enumerate(market_list):
        for b in market_list[i+1:]:
            # Check if these markets should be correlated
            title_a = a["market"]["title"].lower()
            title_b = b["market"]["title"].lower()

            is_correlated = False
            for kw_a, kw_b in CORRELATION_HINTS:
                if (kw_a in title_a and kw_b in title_b) or \
                   (kw_b in title_a and kw_a in title_b):
                    is_correlated = True
                    break

            # Also use title similarity
            if not is_correlated:
                sim = SequenceMatcher(None, title_a, title_b).ratio()
                if sim > 0.4:
                    is_correlated = True

            if not is_correlated:
                continue

            # Check if one moved significantly but the other didn't
            big_a = abs(a["move"]) >= threshold_pct
            big_b = abs(b["move"]) >= threshold_pct

            if big_a and not big_b and abs(b["move"]) < threshold_pct * 0.3:
                anomalies.append({
                    "mover": a, "laggard": b,
                    "mover_market": a["market"], "laggard_market": b["market"],
                })
            elif big_b and not big_a and abs(a["move"]) < threshold_pct * 0.3:
                anomalies.append({
                    "mover": b, "laggard": a,
                    "mover_market": b["market"], "laggard_market": a["market"],
                })

    return anomalies


def find_big_moves(all_markets, price_history, threshold_pct=None):
    """Find markets with big 24h price moves."""
    if threshold_pct is None:
        threshold_pct = BIG_MOVE_THRESHOLD_PCT

    moves = []
    for m in all_markets:
        mid = m["id"]
        current = m["yes_price"] * 100
        history = price_history.get(mid, [])
        if not history:
            continue

        oldest = history[0]["price"] * 100
        delta = current - oldest

        if abs(delta) >= threshold_pct:
            moves.append({
                "market": m,
                "old_price": round(oldest, 1),
                "new_price": round(current, 1),
                "delta": round(delta, 1),
            })

    moves.sort(key=lambda x: abs(x["delta"]), reverse=True)
    return moves


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STATE PERSISTENCE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class State:
    def __init__(self):
        self.price_history = {}    # market_id â†’ [{"price": 0.72, "ts": "...", ...}, ...]
        self.seen_news = []        # GUIDs of sent news
        self.sent_arb_alerts = {}  # "poly_id:kalshi_id" â†’ last_alert_ts
        self.sent_corr_alerts = {} # "id_a:id_b" â†’ last_alert_ts
        self.sent_move_alerts = {} # market_id â†’ last_alert_ts
        self.last_news_check = None
        self.scan_count = 0
        self.knowledge_base = ""   # AI-maintained summary of what we already know
        self.sent_topics = []      # List of topic summaries already sent
        self._load()

    def _load(self):
        if os.path.exists(STATE_FILE):
            try:
                with open(STATE_FILE) as f:
                    s = json.load(f)
                self.price_history = s.get("price_history", {})
                self.seen_news = s.get("seen_news", [])
                self.sent_arb_alerts = s.get("sent_arb_alerts", {})
                self.sent_corr_alerts = s.get("sent_corr_alerts", {})
                self.sent_move_alerts = s.get("sent_move_alerts", {})
                self.last_news_check = s.get("last_news_check")
                self.scan_count = s.get("scan_count", 0)
                self.knowledge_base = s.get("knowledge_base", "")
                self.sent_topics = s.get("sent_topics", [])
                logger.info(f"State loaded â€” scan #{self.scan_count}, "
                           f"tracking {len(self.price_history)} markets, "
                           f"knowledge: {len(self.knowledge_base)} chars")
            except Exception as e:
                logger.warning(f"State load failed: {e}")

    def save(self):
        try:
            os.makedirs(os.path.dirname(STATE_FILE), exist_ok=True)
            # Trim history to last 24h (keep ~500 data points per market)
            trimmed_history = {}
            cutoff = (datetime.now(timezone.utc) - timedelta(hours=25)).isoformat()
            for mid, entries in self.price_history.items():
                trimmed = [e for e in entries if e.get("ts", "") > cutoff]
                if trimmed:
                    trimmed_history[mid] = trimmed[-500:]  # Max 500 per market

            with open(STATE_FILE, "w") as f:
                json.dump({
                    "price_history": trimmed_history,
                    "seen_news": self.seen_news[-1000:],
                    "sent_arb_alerts": self.sent_arb_alerts,
                    "sent_corr_alerts": self.sent_corr_alerts,
                    "sent_move_alerts": self.sent_move_alerts,
                    "last_news_check": self.last_news_check,
                    "scan_count": self.scan_count,
                    "knowledge_base": self.knowledge_base,
                    "sent_topics": self.sent_topics[-50:],
                }, f)
        except Exception as e:
            logger.error(f"State save failed: {e}")

    def record_price(self, market_id: str, price: float):
        """Record a price point for a market."""
        if market_id not in self.price_history:
            self.price_history[market_id] = []
        self.price_history[market_id].append({
            "price": price,
            "ts": datetime.now(timezone.utc).isoformat(),
        })

    def can_alert_arb(self, key: str, cooldown_minutes: int = 30) -> bool:
        """Check if we can send another arbitrage alert (cooldown)."""
        last = self.sent_arb_alerts.get(key)
        if not last:
            return True
        try:
            last_dt = datetime.fromisoformat(last)
            return datetime.now(timezone.utc) - last_dt > timedelta(minutes=cooldown_minutes)
        except (TypeError, ValueError):
            return True

    def mark_arb_alert(self, key: str):
        self.sent_arb_alerts[key] = datetime.now(timezone.utc).isoformat()

    def can_alert_corr(self, key: str, cooldown_minutes: int = 60) -> bool:
        last = self.sent_corr_alerts.get(key)
        if not last:
            return True
        try:
            last_dt = datetime.fromisoformat(last)
            return datetime.now(timezone.utc) - last_dt > timedelta(minutes=cooldown_minutes)
        except (TypeError, ValueError):
            return True

    def mark_corr_alert(self, key: str):
        self.sent_corr_alerts[key] = datetime.now(timezone.utc).isoformat()

    def can_alert_move(self, market_id: str, cooldown_minutes: int = 120) -> bool:
        last = self.sent_move_alerts.get(market_id)
        if not last:
            return True
        try:
            last_dt = datetime.fromisoformat(last)
            return datetime.now(timezone.utc) - last_dt > timedelta(minutes=cooldown_minutes)
        except (TypeError, ValueError):
            return True

    def mark_move_alert(self, market_id: str):
        self.sent_move_alerts[market_id] = datetime.now(timezone.utc).isoformat()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TELEGRAM NOTIFIER (HEBREW)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class Notifier:
    def __init__(self):
        self.bot = Bot(token=TELEGRAM_BOT_TOKEN)
        self.chat = TELEGRAM_CHAT_ID

    async def send(self, text: str) -> bool:
        try:
            # Split if too long (Telegram limit ~4096)
            if len(text) > 4000:
                parts = [text[i:i+4000] for i in range(0, len(text), 4000)]
                for part in parts:
                    await self.bot.send_message(chat_id=self.chat, text=part,
                                                disable_web_page_preview=True)
                    await asyncio.sleep(0.5)
                return True
            await self.bot.send_message(chat_id=self.chat, text=text,
                                        disable_web_page_preview=True)
            return True
        except RetryAfter as e:
            await asyncio.sleep(e.retry_after)
            return await self.send(text)
        except TelegramError as e:
            logger.error(f"TG error: {e}")
            return False

    async def send_startup(self, market_count: int, has_memory: bool = False):
        memory_status = "âœ… ×¤×¢×™×œ" if has_memory else "ğŸ†• ×¨×™×§ (×™×ª××œ× ×‘×¡×¨×™×§×” ×”×¨××©×•× ×”)"
        msg = (
            "ğŸ‡®ğŸ‡· ×‘×•×˜ ××•×“×™×¢×™×Ÿ ×©×•×•×§×™ ×”×™××•×¨×™× â€” ××™×¨××Ÿ\n"
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
            "ğŸ” ××¦×‘: ×¤×¢×™×œ\n"
            f"ğŸ“Š ×©×•×•×§×™× ×¤×¢×™×œ×™×: {market_count}\n"
            f"â° ×¡×¨×™×§×ª ×©×•×•×§×™×: ×›×œ {MARKET_SCAN_MINUTES} ×“×§×•×ª\n"
            f"ğŸ“° ×¡×¨×™×§×ª ×—×“×©×•×ª: ×›×œ {NEWS_SCAN_MINUTES} ×“×§×•×ª\n"
            f"ğŸ¯ ×¡×£ ××¨×‘×™×˜×¨××–': {ARBITRAGE_THRESHOLD_PCT}%\n"
            f"ğŸ“ˆ ×¡×£ ×ª× ×•×¢×” ×’×“×•×œ×”: {BIG_MOVE_THRESHOLD_PCT}%\n"
            f"ğŸ”— ×¡×£ ×§×•×¨×œ×¦×™×”: {CORRELATION_MOVE_PCT}%\n"
            f"ğŸ§  ×× ×•×¢ AI: {'âœ…' if ANTHROPIC_API_KEY else 'âŒ'}\n"
            f"ğŸ’¾ ×–×™×›×¨×•×Ÿ AI: {memory_status}\n"
            "ğŸŒ ×©×¤×”: ×¢×‘×¨×™×ª\n\n"
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        )
        await self.send(msg)

    async def send_arbitrage(self, opp: dict, ai: dict):
        confidence = ai.get("confidence", "â€”")
        msg = (
            "âš–ï¸ ×”×ª×¨××ª ××¨×‘×™×˜×¨××–'\n"
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
            f"ğŸ“Š {ai.get('title_he', opp['name'])}\n\n"
            f"Polymarket: {opp['poly_price']}%\n"
            f"Kalshi: {opp['kalshi_price']}%\n"
            f"ğŸ“ ×¤×¢×¨: {opp['gap_pct']}%\n\n"
        )
        if ai.get("context"):
            msg += f"ğŸ“‹ ×”×§×©×¨:\n{ai['context']}\n\n"
        if ai.get("why_gap"):
            msg += f"â“ ×œ××” ×§×™×™× ×”×¤×¢×¨:\n{ai['why_gap']}\n\n"
        if ai.get("opportunity"):
            msg += f"ğŸ’° ×”×–×“×× ×•×ª:\n{ai['opportunity']}\n\n"
        if ai.get("risk_assessment"):
            msg += f"âš ï¸ ×¡×™×›×•× ×™×:\n{ai['risk_assessment']}\n\n"
        if ai.get("recommendation"):
            msg += f"ğŸ’¡ ×”××œ×¦×”:\n{ai['recommendation']}\n\n"
        if ai.get("watch_factors"):
            factors = "\n".join([f"  â€¢ {f}" for f in ai["watch_factors"]])
            msg += f"ğŸ‘ï¸ ×’×•×¨××™× ×œ×¢×§×•×‘:\n{factors}\n\n"

        msg += (
            f"ğŸ¯ ×¨××ª ×‘×™×˜×—×•×Ÿ: {confidence}\n\n"
            f"ğŸ”— Polymarket: {opp['poly_url']}\n"
            f"ğŸ”— Kalshi: {opp['kalshi_url']}\n"
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        )
        await self.send(msg)

    async def send_correlation(self, anomaly: dict, ai: dict):
        mover = anomaly["mover"]
        laggard = anomaly["laggard"]
        confidence = ai.get("confidence", "â€”")

        msg = (
            "ğŸ”— ×”×ª×¨××ª ×§×•×¨×œ×¦×™×” ×—×¨×™×’×”\n"
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
            f"ğŸ“Š {ai.get('title_he', '×× ×•××œ×™×™×ª ×§×•×¨×œ×¦×™×”')}\n\n"
            f"×©×•×§ ×©×–×–: {mover['market']['title']}\n"
            f"  ××—×™×¨: {mover['current']:.1f}% | ×ª× ×•×¢×”: {mover['move']:+.1f}%\n\n"
            f"×©×•×§ ×©×œ× ×”×’×™×‘: {laggard['market']['title']}\n"
            f"  ××—×™×¨: {laggard['current']:.1f}% | ×ª× ×•×¢×”: {laggard['move']:+.1f}%\n\n"
        )
        if ai.get("context"):
            msg += f"ğŸ“‹ ×”×§×©×¨:\n{ai['context']}\n\n"
        if ai.get("anomaly_explanation"):
            msg += f"ğŸ” ×”×¡×‘×¨ ×”×× ×•××œ×™×”:\n{ai['anomaly_explanation']}\n\n"
        if ai.get("opportunity"):
            msg += f"ğŸ’° ×”×–×“×× ×•×ª:\n{ai['opportunity']}\n\n"
        if ai.get("recommendation"):
            msg += f"ğŸ’¡ ×”××œ×¦×”:\n{ai['recommendation']}\n\n"
        if ai.get("watch_factors"):
            factors = "\n".join([f"  â€¢ {f}" for f in ai["watch_factors"]])
            msg += f"ğŸ‘ï¸ ×’×•×¨××™× ×œ×¢×§×•×‘:\n{factors}\n\n"

        msg += (
            f"ğŸ¯ ×¨××ª ×‘×™×˜×—×•×Ÿ: {confidence}\n"
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        )
        await self.send(msg)

    async def send_big_move(self, move: dict, ai: dict):
        m = move["market"]
        arrow = "ğŸ“ˆ" if move["delta"] > 0 else "ğŸ“‰"
        confidence = ai.get("confidence", "â€”")
        news_based = ai.get("news_based", False)
        source_tag = "ğŸ“° ××‘×•×¡×¡ ×—×“×©×•×ª" if news_based else "âš ï¸ ×”×¡×™×‘×” ×œ× ×‘×¨×•×¨×”"

        msg = (
            f"{arrow} ×ª× ×•×¢×” ×’×“×•×œ×”\n"
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
            f"ğŸ“Š {ai.get('title_he', m['title'])}\n"
            f"××§×•×¨: {m['source']}\n"
            f"{source_tag}\n\n"
            f"×œ×¤× ×™: {move['old_price']}% â†’ ×¢×›×©×™×•: {move['new_price']}%\n"
            f"×©×™× ×•×™: {move['delta']:+.1f}%\n\n"
        )
        if ai.get("cause"):
            msg += f"â“ ×œ××” ×–×” ×§×¨×”:\n{ai['cause']}\n\n"
        if ai.get("forward_prediction"):
            msg += f"ğŸ”® ×ª×—×–×™×ª ×§×“×™××”:\n{ai['forward_prediction']}\n\n"
        if ai.get("related_markets_prediction"):
            msg += f"ğŸ”— ×©×•×•×§×™× ×©×¦×¤×•×™×™× ×œ×–×•×–:\n{ai['related_markets_prediction']}\n\n"
        if ai.get("action_window"):
            msg += f"â° ×—×œ×•×Ÿ ×¤×¢×•×œ×”: {ai['action_window']}\n\n"
        if ai.get("recommendation"):
            msg += f"ğŸ’¡ ×”××œ×¦×”:\n{ai['recommendation']}\n\n"
        if ai.get("watch_factors"):
            factors = "\n".join([f"  â€¢ {f}" for f in ai["watch_factors"]])
            msg += f"ğŸ‘ï¸ ×’×•×¨××™× ×œ×¢×§×•×‘:\n{factors}\n\n"

        msg += (
            f"ğŸ¯ ×¨××ª ×‘×™×˜×—×•×Ÿ: {confidence}\n\n"
            f"ğŸ”— {m['url']}\n"
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        )
        await self.send(msg)

    async def send_news(self, ai: dict, news_items: list):
        urgency = ai.get("urgency", "×œ×™×“×™×¢×”")
        urgency_emoji = {"×“×—×•×£": "ğŸš¨", "×—×©×•×‘": "âš ï¸", "×œ×™×“×™×¢×”": "â„¹ï¸"}.get(urgency, "â„¹ï¸")
        confidence = ai.get("overall_confidence", ai.get("confidence", "â€”"))

        msg = (
            f"{urgency_emoji} ×ª×—×–×™×ª ×©×•×•×§×™× â€” ××™×¨××Ÿ\n"
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
            f"ğŸ“Œ {ai.get('headline_he', '×¢×“×›×•×Ÿ ×—×“×©')}\n\n"
        )
        if ai.get("news_summary_he"):
            msg += f"ğŸ“° ××” ×§×¨×”: {ai['news_summary_he']}\n\n"

        # Predictions - the main event
        predictions = ai.get("predictions", [])
        if predictions:
            msg += "ğŸ”® ×ª×—×–×™×•×ª ×ª× ×•×¢×”:\n"
            for pred in predictions[:5]:
                direction = pred.get("direction", "â€”")
                dir_emoji = "ğŸ“ˆ" if direction == "×¢×œ×™×™×”" else "ğŸ“‰"
                conf = pred.get("confidence", "â€”")
                msg += (
                    f"\n  {dir_emoji} {pred.get('market', 'â€”')}\n"
                    f"    ×¢×›×©×™×•: {pred.get('current_price', 'â€”')} â†’ ×¦×¤×™: {pred.get('predicted_price', 'â€”')}\n"
                    f"    ğŸ¯ ×˜×¨×™×’×¨: {pred.get('trigger_event', 'â€”')}\n"
                    f"    â° {pred.get('timeframe', 'â€”')} | ×‘×™×˜×—×•×Ÿ: {conf}\n"
                    f"    ğŸ’¬ {pred.get('logic', '')}\n"
                )
            msg += "\n"

        if ai.get("opportunity"):
            msg += f"ğŸ’° ×”×–×“×× ×•×ª:\n{ai['opportunity']}\n\n"
        if ai.get("risk_warning"):
            msg += f"âš ï¸ ×¡×™×›×•× ×™×:\n{ai['risk_warning']}\n\n"

        action_items = ai.get("action_items", [])
        if action_items:
            items = "\n".join([f"  âœ… {a}" for a in action_items])
            msg += f"ğŸ“‹ ×¤×¢×•×œ×•×ª ××•××œ×¦×•×ª:\n{items}\n\n"

        # Source links
        msg += "ğŸ”— ××§×•×¨×•×ª:\n"
        for item in news_items[:3]:
            msg += f"  â€¢ {item['source']}: {item['link']}\n"

        msg += (
            f"\nğŸ¯ ×¨××ª ×‘×™×˜×—×•×Ÿ ×›×•×œ×œ×ª: {confidence}\n"
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        )
        await self.send(msg)

    async def send_market_snapshot(self, markets: list):
        """Send periodic market snapshot."""
        if not markets:
            return
        msg = "ğŸ“Š ××¦×‘ ×©×•×•×§×™× â€” ××™×¨××Ÿ\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
        for m in markets[:20]:
            pct = m["yes_price"] * 100
            msg += f"  {'â—' if pct > 50 else 'â—‹'} {m['title']}\n    {m['source']}: {pct:.1f}%\n\n"
        msg += "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        await self.send(msg)

    async def send_heartbeat(self, market_count: int, scan_count: int):
        ts = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M UTC")
        msg = (
            f"ğŸ’“ ×”×‘×•×˜ ×¤×¢×™×œ â€” {ts}\n"
            f"×©×•×•×§×™× ×‘××¢×§×‘: {market_count}\n"
            f"×¡×¨×™×§×•×ª ×©×‘×•×¦×¢×•: {scan_count}\n"
            f"ğŸ§  AI: {'âœ…' if ANTHROPIC_API_KEY else 'âŒ'}"
        )
        await self.send(msg)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN SCAN LOOPS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Global state and notifier
state: Optional[State] = None
notifier: Optional[Notifier] = None


async def market_scan():
    """Main market scan â€” runs every 2-3 minutes."""
    global state, notifier
    state.scan_count += 1
    logger.info(f"â•â•â• Market Scan #{state.scan_count} â•â•â•")

    try:
        async with aiohttp.ClientSession() as s:
            # â”€â”€ Fetch all Iran markets â”€â”€
            poly = await fetch_polymarket_iran(s)
            kalshi = await fetch_kalshi_iran(s)
            all_markets = poly + kalshi

            if not all_markets:
                logger.info("No Iran markets found this scan")
                state.save()
                return

            # â”€â”€ Record prices â”€â”€
            for m in all_markets:
                state.record_price(m["id"], m["yes_price"])

            # â”€â”€ 1. Arbitrage Detection â”€â”€
            arb_opps = find_arbitrage_opportunities(poly, kalshi)
            for opp in arb_opps[:3]:
                alert_key = f"{opp['name'][:30]}"
                if not state.can_alert_arb(alert_key):
                    continue
                logger.info(f"âš–ï¸ ARBITRAGE: {opp['name']} â€” {opp['gap_pct']}%")
                ai = await ai_analyze_arbitrage(
                    s, opp["name"], opp["poly_price"], opp["kalshi_price"],
                    opp["gap_pct"], opp["poly_url"], opp["kalshi_url"]
                )
                if not ai:
                    ai = {"title_he": opp["name"]}
                await notifier.send_arbitrage(opp, ai)
                state.mark_arb_alert(alert_key)

            # â”€â”€ 2. Big Moves â€” fetch news to explain WHY â”€â”€
            big_moves = find_big_moves(all_markets, state.price_history)
            if big_moves:
                # Fetch fresh news to explain the moves
                move_news = await fetch_iran_news(s)
                logger.info(f"Fetched {len(move_news)} news items to explain {len(big_moves)} moves")
            else:
                move_news = []

            for move in big_moves[:3]:
                mid = move["market"]["id"]
                if not state.can_alert_move(mid):
                    continue
                logger.info(f"ğŸ“ˆ BIG MOVE: {move['market']['title']} â€” {move['delta']:+.1f}%")
                ai = await ai_analyze_big_move(
                    s, move["market"], move["old_price"], move["new_price"],
                    "24 ×©×¢×•×ª", recent_news=move_news
                )
                if not ai:
                    ai = {"title_he": move["market"]["title"]}
                await notifier.send_big_move(move, ai)
                state.mark_move_alert(mid)

            # â”€â”€ 3. Correlation Anomalies â”€â”€
            anomalies = find_correlation_anomalies(all_markets, state.price_history)
            for anomaly in anomalies[:2]:
                key = f"{anomaly['mover_market']['id']}:{anomaly['laggard_market']['id']}"
                if not state.can_alert_corr(key):
                    continue
                logger.info(f"ğŸ”— CORRELATION: {anomaly['mover_market']['title']} vs {anomaly['laggard_market']['title']}")
                ai = await ai_analyze_correlation(
                    s,
                    anomaly["mover_market"], anomaly["laggard_market"],
                    anomaly["mover"]["current"], anomaly["laggard"]["current"],
                    anomaly["mover"]["move"], anomaly["laggard"]["move"],
                )
                if not ai:
                    ai = {"title_he": "×× ×•××œ×™×™×ª ×§×•×¨×œ×¦×™×”"}
                await notifier.send_correlation(anomaly, ai)
                state.mark_corr_alert(key)

            # â”€â”€ 4. Periodic snapshot (every ~30 min = 10 scans) â”€â”€
            if state.scan_count % 10 == 0:
                await notifier.send_market_snapshot(all_markets)

            # â”€â”€ 5. Heartbeat (every ~6h = 120 scans) â”€â”€
            if state.scan_count % 120 == 0:
                await notifier.send_heartbeat(len(all_markets), state.scan_count)

            state.save()
            logger.info(f"Market scan done â€” {len(all_markets)} markets tracked")

    except Exception as e:
        logger.exception(f"Market scan error: {e}")
        try:
            await notifier.send(f"âš ï¸ ×©×’×™××” ×‘×¡×¨×™×§×ª ×©×•×•×§×™×: {str(e)[:300]}")
        except Exception:
            pass


async def news_scan():
    """News scan â€” runs every 5-10 minutes. Uses AI memory to avoid duplicates."""
    global state, notifier
    logger.info("â•â•â• News Scan â•â•â•")

    try:
        async with aiohttp.ClientSession() as s:
            # Fetch news
            news = await fetch_iran_news(s)

            # Stage 1: Filter out already seen GUIDs
            new_items = [n for n in news if n["guid"] not in state.seen_news]
            if not new_items:
                logger.info("No new Iran news (all GUIDs seen)")
                return

            new_items = new_items[:10]  # Take top 10 for AI filtering

            # Stage 2: AI FILTER â€” Claude decides what's truly new
            logger.info(f"ğŸ“° {len(new_items)} new GUIDs â€” asking AI to filter...")
            filtered = await ai_filter_news(s, new_items, state.knowledge_base, state.sent_topics)

            # Mark ALL fetched items as seen (even filtered ones)
            for item in new_items:
                state.seen_news.append(item["guid"])

            if not filtered:
                logger.info("AI filter: nothing truly new â€” all filtered out")
                state.save()
                return

            logger.info(f"AI filter: {len(filtered)} items passed (out of {len(new_items)})")

            # Stage 3: Get current markets for context
            poly = await fetch_polymarket_iran(s)
            kalshi = await fetch_kalshi_iran(s)
            all_markets = poly + kalshi

            # Stage 4: AI analysis WITH knowledge base context
            ai = await ai_analyze_news(s, filtered, all_markets, state.knowledge_base)

            if ai:
                await notifier.send_news(ai, filtered)

                # Stage 5: Update knowledge base with new info
                topic_summary = ai.get("topic_summary", "")
                if topic_summary:
                    state.sent_topics.append(topic_summary)

                summary_he = ai.get("news_summary_he", "")
                if summary_he:
                    news_titles = [item["title"] for item in filtered]
                    updated_kb = await ai_update_knowledge(
                        s, state.knowledge_base, summary_he, news_titles
                    )
                    if updated_kb:
                        state.knowledge_base = updated_kb
                        logger.info(f"Knowledge base updated ({len(updated_kb)} chars)")
            else:
                # Fallback: send raw (only first item)
                item = filtered[0]
                msg = (
                    f"ğŸ“° ×—×“×©×•×ª ××™×¨××Ÿ\n\n"
                    f"ğŸ“Œ {item['title']}\n"
                    f"××§×•×¨: {item['source']}\n"
                    f"ğŸ”— {item['link']}"
                )
                await notifier.send(msg)

            state.last_news_check = datetime.now(timezone.utc).isoformat()
            state.save()

    except Exception as e:
        logger.exception(f"News scan error: {e}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENTRY POINT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def main():
    global state, notifier

    state = State()
    notifier = Notifier()

    logger.info("ğŸ‡®ğŸ‡· Starting Iran Market Intelligence Bot...")

    # Initial fetch to show market count
    async with aiohttp.ClientSession() as s:
        poly = await fetch_polymarket_iran(s)
        kalshi = await fetch_kalshi_iran(s)
        initial_count = len(poly) + len(kalshi)
        # Record initial prices
        for m in poly + kalshi:
            state.record_price(m["id"], m["yes_price"])
        state.save()

    await notifier.send_startup(initial_count, has_memory=bool(state.knowledge_base))

    if "--once" in sys.argv:
        await market_scan()
        await news_scan()
        return

    # Set up scheduler with different intervals
    scheduler = AsyncIOScheduler()

    scheduler.add_job(
        market_scan,
        IntervalTrigger(minutes=MARKET_SCAN_MINUTES),
        id="market_scan",
        name="Iran Market Scan",
        max_instances=1,
        misfire_grace_time=120,
    )

    scheduler.add_job(
        news_scan,
        IntervalTrigger(minutes=NEWS_SCAN_MINUTES),
        id="news_scan",
        name="Iran News Scan",
        max_instances=1,
        misfire_grace_time=120,
    )

    scheduler.start()

    # Run first scans immediately
    await market_scan()
    await asyncio.sleep(5)
    await news_scan()

    logger.info(f"Scheduler active â€” Markets every {MARKET_SCAN_MINUTES}min, "
                f"News every {NEWS_SCAN_MINUTES}min")

    stop = asyncio.Event()

    def handle_sig(sig, frame):
        logger.info("Shutting down...")
        stop.set()

    signal.signal(signal.SIGINT, handle_sig)
    signal.signal(signal.SIGTERM, handle_sig)

    await stop.wait()
    scheduler.shutdown(wait=False)
    logger.info("Bot stopped.")


if __name__ == "__main__":
    asyncio.run(main())
