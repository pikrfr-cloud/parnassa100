#!/usr/bin/env python3
"""
ğŸš€ ×‘×•×˜ ××•×“×™×¢×™×Ÿ ×©×•×•×§×™× â€” ×’×¨×¡×” ×¢×‘×¨×™×ª
=====================================
×× ×˜×¨ Polymarket, Kalshi ×•-RSS feeds.
×©×•×œ×— ×”×ª×¨××•×ª ×‘×¢×‘×¨×™×ª ×œ×˜×œ×’×¨× ×¢× × ×™×ª×•×— ×”×©×¤×¢×” ×¢×œ ×”×™××•×¨×™×.
"""

import asyncio
import json
import logging
import os
import re
import signal
import sys
from dataclasses import dataclass
from datetime import datetime, timezone
from difflib import SequenceMatcher
from typing import Any, Optional

import aiohttp
import feedparser
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger
from dotenv import load_dotenv
from telegram import Bot
from telegram.error import TelegramError, RetryAfter

load_dotenv()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN", "")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID", "")
CHECK_INTERVAL = int(os.getenv("CHECK_INTERVAL_MINUTES", "120"))
ALERT_THRESHOLD = int(os.getenv("ALERT_THRESHOLD_BPS", "15"))
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
STATE_FILE = os.getenv("STATE_FILE", "/data/bot_state.json")

POLYMARKET_API = "https://gamma-api.polymarket.com"
KALSHI_API = "https://api.elections.kalshi.com/trade-api/v2"

RSS_FEEDS = {
    "central_banks": [
        {"name": "×”×¤×“×¨×œ ×¨×™×–×¨×‘", "url": "https://www.federalreserve.gov/feeds/press_all.xml"},
        {"name": "×”×‘× ×§ ×”××™×¨×•×¤×™", "url": "https://www.ecb.europa.eu/rss/press.html"},
    ],
    "news": [
        {"name": "CoinDesk", "url": "https://www.coindesk.com/arc/outboundfeeds/rss/"},
        {"name": "Politico", "url": "https://rss.politico.com/politics-news.xml"},
    ],
    "legislation": [
        {"name": "×”×§×•× ×’×¨×¡ ×”×××¨×™×§××™", "url": "https://www.govinfo.gov/rss/bills.xml"},
    ],
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOGGING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

logging.basicConfig(
    level=getattr(logging, LOG_LEVEL, logging.INFO),
    format="%(asctime)s â”‚ %(levelname)-7s â”‚ %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger("bot")
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("telegram").setLevel(logging.WARNING)
logging.getLogger("apscheduler").setLevel(logging.WARNING)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CATEGORY & KEYWORD CONFIG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CATEGORY_KEYWORDS = {
    "crypto": ["bitcoin", "btc", "eth", "ethereum", "crypto", "solana", "defi"],
    "politics": ["president", "election", "trump", "biden", "senate", "congress", "vote", "governor"],
    "macro": ["fed", "rate", "inflation", "gdp", "recession", "treasury", "cpi", "tariff", "interest rate"],
    "sports": ["nba", "nfl", "mlb", "super bowl", "world cup", "champion"],
    "tech": ["ai", "openai", "apple", "google", "microsoft", "tesla", "spacex"],
    "climate": ["hurricane", "earthquake", "temperature", "climate", "wildfire"],
}

CATEGORY_HEBREW = {
    "crypto": "×§×¨×™×¤×˜×•",
    "politics": "×¤×•×œ×™×˜×™×§×”",
    "macro": "×××§×¨×•/×¨×™×‘×™×•×ª",
    "sports": "×¡×¤×•×¨×˜",
    "tech": "×˜×›× ×•×œ×•×’×™×”",
    "climate": "××§×œ×™×",
    "other": "××—×¨",
}

RSS_KEYWORDS = [
    "interest rate", "rate decision", "monetary policy", "inflation", "cpi", "gdp",
    "recession", "fed", "ecb", "fomc", "bitcoin", "crypto", "stablecoin",
    "election", "legislation", "bill pass", "executive order", "sanction", "tariff",
    "war", "conflict", "ceasefire", "breaking", "urgent", "surprise",
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRANSLATION DICTIONARY (common market terms ENâ†’HE)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TERM_TRANSLATIONS = {
    # Politics
    "president": "× ×©×™×", "election": "×‘×—×™×¨×•×ª", "senate": "×¡× ××˜",
    "congress": "×§×•× ×’×¨×¡", "vote": "×”×¦×‘×¢×”", "governor": "××•×©×œ",
    "impeach": "×”×“×—×”", "democrat": "×“××•×§×¨×˜×™×", "republican": "×¨×¤×•×‘×œ×™×§× ×™×",
    "white house": "×”×‘×™×ª ×”×œ×‘×Ÿ", "supreme court": "×‘×™×ª ×”××©×¤×˜ ×”×¢×œ×™×•×Ÿ",
    # Macro
    "interest rate": "×¨×™×‘×™×ª", "rate cut": "×”×•×¨×“×ª ×¨×™×‘×™×ª", "rate hike": "×”×¢×œ××ª ×¨×™×‘×™×ª",
    "inflation": "××™× ×¤×œ×¦×™×”", "recession": "××™×ª×•×Ÿ", "gdp": "×ª×•×¦×¨ ××§×•××™ ×’×•×œ××™",
    "unemployment": "××‘×˜×œ×”", "tariff": "××›×¡", "trade war": "××œ×—××ª ×¡×—×¨",
    "debt ceiling": "×ª×§×¨×ª ×—×•×‘", "federal reserve": "×”×¤×“×¨×œ ×¨×™×–×¨×‘",
    "central bank": "×‘× ×§ ××¨×›×–×™", "monetary policy": "××“×™× ×™×•×ª ××•× ×™×˜×¨×™×ª",
    # Crypto
    "bitcoin": "×‘×™×˜×§×•×™×Ÿ", "ethereum": "××ª×¨×™×•×", "crypto": "×§×¨×™×¤×˜×•",
    "stablecoin": "××˜×‘×¢ ×™×¦×™×‘", "halving": "×—×¦×™×™×”", "etf": "×ª×¢×•×“×ª ×¡×œ",
    "token": "×˜×•×§×Ÿ", "blockchain": "×‘×œ×•×§×¦'×™×™×Ÿ",
    # Geopolitics
    "war": "××œ×—××”", "ceasefire": "×”×¤×¡×§×ª ××©", "conflict": "×¡×›×¡×•×š",
    "sanction": "×¡× ×§×¦×™×”", "invasion": "×¤×œ×™×©×”", "missile": "×˜×™×œ",
    "nato": "× ××˜\"×•",
    # General
    "yes": "×›×Ÿ", "no": "×œ×", "will": "×”××",
    "before": "×œ×¤× ×™", "after": "××—×¨×™", "by": "×¢×“",
    "win": "× ×™×¦×—×•×Ÿ", "lose": "×”×¤×¡×“", "above": "××¢×œ", "below": "××ª×—×ª",
}


def translate_title(title: str) -> str:
    """Translate an English market title to Hebrew (keyword-based)."""
    result = title
    for en, he in sorted(TERM_TRANSLATIONS.items(), key=lambda x: -len(x[0])):
        pattern = re.compile(re.escape(en), re.IGNORECASE)
        result = pattern.sub(he, result)
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BETTING IMPACT ANALYSIS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Map event keywords to related betting markets and impact level
IMPACT_RULES = [
    # Macro / Central Banks
    {
        "triggers": ["rate cut", "rate decision", "interest rate", "fed", "fomc", "monetary policy", "dovish", "hawkish"],
        "markets": ["×©×•×§×™ ×¨×™×‘×™×•×ª (Kalshi/Poly)", "××’\"×— ×××©×œ×ª×™", "××˜\"×— (×“×•×œ×¨)", "×× ×™×•×ª ×¦××™×—×”"],
        "level": "ğŸ”´ ×’×‘×•×”×”",
        "note": "×”×—×œ×˜×•×ª ×¨×™×‘×™×ª ××©×¤×™×¢×•×ª ×™×©×™×¨×•×ª ×¢×œ ×©×•×•×§×™ ×”×ª×—×–×™×•×ª ×©×œ ×¨×™×‘×™×•×ª, ××’\"×—, ×•×“×•×œ×¨",
    },
    {
        "triggers": ["inflation", "cpi", "pce"],
        "markets": ["×©×•×§×™ ×¨×™×‘×™×•×ª", "×”×™××•×¨×™ ××“×™× ×™×•×ª ×”×¤×“", "×¡×—×•×¨×•×ª"],
        "level": "ğŸ”´ ×’×‘×•×”×”",
        "note": "× ×ª×•× ×™ ××™× ×¤×œ×¦×™×” ××–×™×–×™× ×¦×™×¤×™×•×ª ×¨×™×‘×™×ª ×•×©×•×•×§×™ ×ª×—×–×™×•×ª",
    },
    {
        "triggers": ["recession", "gdp", "unemployment", "payroll", "jobs"],
        "markets": ["×”×™××•×¨×™ ××™×ª×•×Ÿ (Poly/Kalshi)", "×©×•×§×™ ×× ×™×•×ª", "××’\"×—"],
        "level": "ğŸŸ¡ ×‘×™× ×•× ×™×ª-×’×‘×•×”×”",
        "note": "× ×ª×•× ×™ ×ª×¢×¡×•×§×” ×•×¦××™×—×” ××©×¤×™×¢×™× ×¢×œ ×”×™××•×¨×™ ××™×ª×•×Ÿ ×•×”×¨×’×©×ª ×”×©×•×§",
    },
    {
        "triggers": ["tariff", "trade war", "trade deal", "import tax"],
        "markets": ["×”×™××•×¨×™ ××œ×—××ª ×¡×—×¨", "×©×•×•×§×™ ×× ×™×•×ª ×‘×™× ×œ××•××™×™×", "××˜\"×—"],
        "level": "ğŸŸ¡ ×‘×™× ×•× ×™×ª-×’×‘×•×”×”",
        "note": "××›×¡×™× ×™×›×•×œ×™× ×œ×–×¢×–×¢ ×©×•×•×§×™× ×•×œ×”×©×¤×™×¢ ×¢×œ ×”×™××•×¨×™ ×¡×—×¨ ×•××˜\"×—",
    },
    # Crypto
    {
        "triggers": ["bitcoin", "btc", "crypto", "ethereum", "eth"],
        "markets": ["×”×™××•×¨×™ ××—×™×¨ ×‘×™×˜×§×•×™×Ÿ", "×”×™××•×¨×™ ETF ×§×¨×™×¤×˜×•", "××œ×˜×§×•×™× ×™×"],
        "level": "ğŸŸ¡ ×‘×™× ×•× ×™×ª",
        "note": "×—×“×©×•×ª ×§×¨×™×¤×˜×• ××©×¤×™×¢×•×ª ×¢×œ ×©×•×•×§×™ ×ª×—×–×™×•×ª ××—×™×¨×™× ×•×¨×’×•×œ×¦×™×”",
    },
    {
        "triggers": ["etf approval", "sec crypto", "crypto regulation", "stablecoin bill"],
        "markets": ["×”×™××•×¨×™ ××™×©×•×¨ ETF", "×”×™××•×¨×™ ×¨×’×•×œ×¦×™×”", "××—×™×¨×™ ×§×¨×™×¤×˜×•"],
        "level": "ğŸ”´ ×’×‘×•×”×”",
        "note": "×”×—×œ×˜×•×ª ×¨×’×•×œ×¦×™×” ××©× ×•×ª ××ª ×©×•×§ ×”×§×¨×™×¤×˜×• ×‘××•×¤×Ÿ ××”×•×ª×™",
    },
    # Politics / Elections
    {
        "triggers": ["election", "poll", "primary", "ballot", "swing state"],
        "markets": ["×”×™××•×¨×™ ×‘×—×™×¨×•×ª (Poly/Kalshi)", "×”×™××•×¨×™ ××“×™× ×•×ª ××¤×ª×—", "×”×™××•×¨×™ ×¡× ××˜"],
        "level": "ğŸ”´ ×’×‘×•×”×”",
        "note": "×¢×“×›×•× ×™ ×‘×—×™×¨×•×ª ××©×¤×™×¢×™× ×™×©×™×¨×•×ª ×¢×œ ×©×•×§×™ ×”×”×™××•×¨×™× ×”×¤×•×œ×™×˜×™×™×",
    },
    {
        "triggers": ["impeach", "resign", "scandal", "indictment", "trial"],
        "markets": ["×”×™××•×¨×™ ×”×“×—×”/×”×ª×¤×˜×¨×•×ª", "×”×™××•×¨×™ ×‘×—×™×¨×•×ª", "×©×•×§×™ ×× ×™×•×ª"],
        "level": "ğŸŸ¡ ×‘×™× ×•× ×™×ª-×’×‘×•×”×”",
        "note": "××™×¨×•×¢×™× ××©×¤×˜×™×™×/×¤×•×œ×™×˜×™×™× ×™×›×•×œ×™× ×œ×©× ×•×ª ×¡×™×›×•×™×™ ××•×¢××“×™×",
    },
    {
        "triggers": ["legislation", "bill pass", "executive order", "congress vote", "senate vote"],
        "markets": ["×”×™××•×¨×™ ×—×§×™×§×”", "×”×™××•×¨×™× ×¢× ×¤×™×™× ×¨×œ×•×•× ×˜×™×™×"],
        "level": "ğŸŸ¡ ×‘×™× ×•× ×™×ª",
        "note": "×—×§×™×§×” ×—×“×©×” ×™×›×•×œ×” ×œ×¤×ª×•×— ××• ×œ×¡×’×•×¨ ×©×•×•×§×™ ×”×™××•×¨×™×",
    },
    # Geopolitics
    {
        "triggers": ["war", "invasion", "conflict", "attack", "missile", "military"],
        "markets": ["×”×™××•×¨×™ ×’×™××•×¤×•×œ×™×˜×™×§×”", "× ×¤×˜ ×•×¡×—×•×¨×•×ª", "×©×•×§×™ ×× ×™×•×ª", "××˜\"×—"],
        "level": "ğŸ”´ ×’×‘×•×”×”",
        "note": "××™×¨×•×¢×™× ×¦×‘××™×™× ×’×•×¨××™× ×œ×ª× ×•×“×ª×™×•×ª ×—×“×” ×‘×›×œ ×”×©×•×•×§×™×",
    },
    {
        "triggers": ["ceasefire", "peace deal", "treaty", "negotiation"],
        "markets": ["×”×™××•×¨×™ ×”×¤×¡×§×ª ××©/×©×œ×•×", "× ×¤×˜", "×©×•×§×™ ×× ×™×•×ª ××–×•×¨×™×™×"],
        "level": "ğŸŸ¡ ×‘×™× ×•× ×™×ª-×’×‘×•×”×”",
        "note": "×”×¤×¡×§×•×ª ××© ×•×”×¡×›××™ ×©×œ×•× ××–×™×–×™× ×©×•×•×§×™ ×ª×—×–×™×•×ª ×’×™××•×¤×•×œ×™×˜×™×™×",
    },
    # Tech
    {
        "triggers": ["ai ", "artificial intelligence", "openai", "chatgpt", "agi"],
        "markets": ["×”×™××•×¨×™ AI (××‘× ×™ ×“×¨×š)", "×× ×™×•×ª ×˜×›× ×•×œ×•×’×™×”"],
        "level": "ğŸŸ¡ ×‘×™× ×•× ×™×ª",
        "note": "×¤×¨×™×¦×•×ª ×“×¨×š ×‘-AI ××©×¤×™×¢×•×ª ×¢×œ ×”×™××•×¨×™ ××‘× ×™ ×“×¨×š ×˜×›× ×•×œ×•×’×™×™×",
    },
    {
        "triggers": ["spacex", "launch", "nasa", "mars", "rocket"],
        "markets": ["×”×™××•×¨×™ ×©×™×’×•×¨×™×/×—×œ×œ", "×”×™××•×¨×™ SpaceX"],
        "level": "ğŸŸ¢ × ××•×›×”-×‘×™× ×•× ×™×ª",
        "note": "××™×¨×•×¢×™ ×—×œ×œ ××©×¤×™×¢×™× ×¢×œ ×”×™××•×¨×™ ×©×™×’×•×¨ ×¡×¤×¦×™×¤×™×™×",
    },
    # Climate
    {
        "triggers": ["hurricane", "earthquake", "wildfire", "flood", "storm"],
        "markets": ["×”×™××•×¨×™ ××§×œ×™×/××–×’ ××•×•×™×¨", "×‘×™×˜×•×—", "×¡×—×•×¨×•×ª ×—×§×œ××™×•×ª"],
        "level": "ğŸŸ¡ ×‘×™× ×•× ×™×ª",
        "note": "××™×¨×•×¢×™ ××–×’ ××•×•×™×¨ ×§×™×¦×•× ×™×™× ××©×¤×™×¢×™× ×¢×œ ×”×™××•×¨×™ ××§×œ×™× ×•×¡×—×•×¨×•×ª",
    },
    {
        "triggers": ["sanction", "embargo", "ban"],
        "markets": ["×”×™××•×¨×™ ×¡× ×§×¦×™×•×ª", "× ×¤×˜", "××˜\"×— ×©×œ ××“×™× ×•×ª ××¢×•×¨×‘×•×ª"],
        "level": "ğŸŸ¡ ×‘×™× ×•× ×™×ª-×’×‘×•×”×”",
        "note": "×¡× ×§×¦×™×•×ª ×—×“×©×•×ª ××–×™×–×•×ª ×©×•×•×§×™ ×× ×¨×’×™×” ×•×”×™××•×¨×™ ×’×™××•×¤×•×œ×™×˜×™×§×”",
    },
]


def analyze_impact(title: str, summary: str = "") -> str:
    """Analyze which betting markets could be affected and at what level."""
    text = f"{title} {summary}".lower()
    impacts = []

    for rule in IMPACT_RULES:
        if any(trigger in text for trigger in rule["triggers"]):
            impacts.append(rule)

    if not impacts:
        return ""

    # Deduplicate by level+note
    seen = set()
    unique = []
    for imp in impacts:
        key = imp["note"]
        if key not in seen:
            seen.add(key)
            unique.append(imp)

    lines = ["\nğŸ° *×”×©×¤×¢×” ×¢×œ ×”×™××•×¨×™×:*"]
    for imp in unique[:3]:  # Max 3 impacts per alert
        markets_str = ", ".join(imp["markets"][:4])
        lines.append(f"  {imp['level']} â€” {markets_str}")
        lines.append(f"  ğŸ’¡ {imp['note']}")

    return "\n".join(lines)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HELPERS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def classify(title):
    t = title.lower()
    for cat, kws in CATEGORY_KEYWORDS.items():
        if any(k in t for k in kws):
            return cat
    return "other"


def normalize(title):
    t = title.lower().strip()
    t = re.sub(r"^(will|is|does|has|can)\s+", "", t)
    t = re.sub(r"\?$", "", t)
    return re.sub(r"\s+", " ", t)


def cat_he(cat):
    return CATEGORY_HEBREW.get(cat, "××—×¨")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# POLYMARKET
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def fetch_polymarket(session, limit=100, min_vol=10000):
    events = []
    try:
        url = f"{POLYMARKET_API}/events"
        params = {"active": "true", "closed": "false", "limit": limit,
                  "order": "volume24hr", "ascending": "false"}
        async with session.get(url, params=params, timeout=aiohttp.ClientTimeout(total=30)) as r:
            if r.status != 200:
                logger.error(f"Polymarket API: {r.status}")
                return events
            data = await r.json()

        for ev in data:
            markets = ev.get("markets", [])
            if not markets:
                continue
            vol = sum(float(m.get("volume", 0) or 0) for m in markets)
            if vol < min_vol:
                continue

            primary = markets[0]
            outcomes = []
            op = primary.get("outcomePrices", "")
            if op:
                try:
                    prices = json.loads(op)
                    if len(prices) > 0:
                        outcomes.append({"name": "Yes", "price": float(prices[0])})
                    if len(prices) > 1:
                        outcomes.append({"name": "No", "price": float(prices[1])})
                except (json.JSONDecodeError, IndexError, ValueError):
                    pass

            if not outcomes:
                yp = primary.get("bestAsk") or primary.get("lastTradePrice")
                if yp:
                    y = float(yp)
                    outcomes = [{"name": "Yes", "price": y}, {"name": "No", "price": 1.0 - y}]

            if not outcomes:
                continue

            title = ev.get("title", "Unknown")
            slug = ev.get("slug", "")
            events.append({
                "id": f"poly_{ev.get('id', '')}",
                "title": title,
                "title_he": translate_title(title),
                "category": classify(title),
                "yes_price": outcomes[0]["price"],
                "volume": vol,
                "url": f"https://polymarket.com/event/{slug}" if slug else "",
                "source": "Polymarket",
            })

        logger.info(f"Polymarket: {len(events)} markets")
    except Exception as e:
        logger.error(f"Polymarket error: {e}")
    return events


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# KALSHI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def fetch_kalshi(session, limit=200, min_vol=100):
    markets = []
    try:
        url = f"{KALSHI_API}/markets"
        params = {"limit": limit, "status": "open"}
        headers = {"Accept": "application/json"}
        async with session.get(url, params=params, headers=headers,
                               timeout=aiohttp.ClientTimeout(total=30)) as r:
            if r.status != 200:
                logger.warning(f"Kalshi API: {r.status}")
                return markets
            data = await r.json()

        cat_map = {"Politics": "politics", "Economics": "macro", "Crypto": "crypto",
                   "Climate and Weather": "climate", "Tech and Science": "tech",
                   "Sports": "sports", "Finance": "macro"}

        for m in data.get("markets", []):
            vol = m.get("volume", 0) or 0
            if vol < min_vol:
                continue
            yp = (m.get("yes_ask", 0) or m.get("last_price", 0) or 0) / 100.0
            ticker = m.get("ticker", "")
            title = m.get("title", "Unknown")
            markets.append({
                "id": f"kalshi_{m.get('id', '')}",
                "title": title,
                "title_he": translate_title(title),
                "category": cat_map.get(m.get("category", ""), "other"),
                "yes_price": yp,
                "volume": vol,
                "url": f"https://kalshi.com/markets/{ticker.lower()}" if ticker else "",
                "source": "Kalshi",
                "subtitle": m.get("subtitle"),
            })

        logger.info(f"Kalshi: {len(markets)} markets")
    except Exception as e:
        logger.error(f"Kalshi error: {e}")
    return markets


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RSS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def fetch_rss(session, since=None):
    items = []
    for cat, feeds in RSS_FEEDS.items():
        for fi in feeds:
            try:
                async with session.get(fi["url"], timeout=aiohttp.ClientTimeout(total=20)) as r:
                    if r.status != 200:
                        continue
                    content = await r.text()
                feed = feedparser.parse(content)
                for entry in feed.entries[:15]:
                    title = entry.get("title", "")
                    summary = entry.get("summary", entry.get("description", ""))[:500]
                    link = entry.get("link", "")
                    guid = entry.get("id", entry.get("guid", link))

                    pub = None
                    if hasattr(entry, "published_parsed") and entry.published_parsed:
                        try:
                            pub = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                        except (TypeError, ValueError):
                            pass

                    if since and pub and pub < since:
                        continue

                    text = f"{title} {summary}".lower()
                    if not any(kw in text for kw in RSS_KEYWORDS):
                        continue

                    items.append({
                        "feed": fi["name"],
                        "cat": cat,
                        "title": title,
                        "title_he": translate_title(title),
                        "summary": summary,
                        "summary_he": translate_title(summary),
                        "link": link,
                        "guid": guid,
                        "pub": pub,
                    })
            except Exception as e:
                logger.warning(f"RSS {fi['name']}: {e}")

    logger.info(f"RSS: {len(items)} relevant items")
    return items


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ANALYZER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def match_and_find_gaps(poly, kalshi, threshold=None):
    if threshold is None:
        threshold = ALERT_THRESHOLD
    alerts = []
    used = set()

    for pe in poly:
        best, best_score = None, 0
        for km in kalshi:
            if km["id"] in used:
                continue
            score = SequenceMatcher(None, normalize(pe["title"]), normalize(km["title"])).ratio()
            if pe["category"] == km["category"] and pe["category"] != "other":
                score += 0.1
            if km.get("subtitle"):
                score = max(score, SequenceMatcher(None, normalize(pe["title"]), normalize(km["subtitle"])).ratio())
            if score > best_score:
                best_score, best = score, km

        if best and best_score >= 0.55:
            used.add(best["id"])
            pp, kp = pe["yes_price"] * 100, best["yes_price"] * 100
            gap = abs(pp - kp) * 100
            if gap >= threshold:
                alerts.append({
                    "name": pe["title"],
                    "name_he": pe.get("title_he", pe["title"]),
                    "cat": pe["category"],
                    "poly": round(pp, 1),
                    "kalshi": round(kp, 1),
                    "gap": round(gap),
                    "dir": "Poly ×’×‘×•×” ×™×•×ª×¨" if pp > kp else "Kalshi ×’×‘×•×” ×™×•×ª×¨",
                    "poly_url": pe["url"],
                    "kalshi_url": best["url"],
                })
    alerts.sort(key=lambda a: a["gap"], reverse=True)
    return alerts


def find_big_moves(current, previous, info, threshold=None):
    if threshold is None:
        threshold = ALERT_THRESHOLD
    alerts = []
    for mid, new_p in current.items():
        old_p = previous.get(mid)
        if old_p is None:
            continue
        delta = abs(new_p - old_p) * 100 * 100
        if delta >= threshold:
            i = info.get(mid, {})
            alerts.append({
                "name": i.get("title", mid),
                "name_he": i.get("title_he", i.get("title", mid)),
                "cat": i.get("category", "other"),
                "src": i.get("source", "?"),
                "old": round(old_p * 100, 1),
                "new": round(new_p * 100, 1),
                "delta": round(delta),
                "tf": f"{CHECK_INTERVAL} ×“×§×•×ª",
                "url": i.get("url", ""),
            })
    alerts.sort(key=lambda a: a["delta"], reverse=True)
    return alerts


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STATE PERSISTENCE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class State:
    def __init__(self):
        self.prices = {}
        self.info = {}
        self.seen_guids = []
        self.last_run = None
        self.run_count = 0
        self._load()

    def _load(self):
        if os.path.exists(STATE_FILE):
            try:
                with open(STATE_FILE) as f:
                    s = json.load(f)
                self.prices = s.get("prices", {})
                self.info = s.get("info", {})
                self.seen_guids = s.get("seen_guids", [])
                self.last_run = s.get("last_run")
                self.run_count = s.get("run_count", 0)
                logger.info(f"State loaded â€” run #{self.run_count}")
            except Exception as e:
                logger.warning(f"State load failed: {e}")

    def save(self):
        try:
            os.makedirs(os.path.dirname(STATE_FILE), exist_ok=True)
            with open(STATE_FILE, "w") as f:
                json.dump({"prices": self.prices, "info": self.info,
                           "seen_guids": self.seen_guids[-3000:],
                           "last_run": self.last_run, "run_count": self.run_count}, f)
        except Exception as e:
            logger.error(f"State save failed: {e}")

    def get_last_run_dt(self):
        if self.last_run:
            try:
                return datetime.fromisoformat(self.last_run)
            except (TypeError, ValueError):
                pass
        return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TELEGRAM (HEBREW ONLY)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class Notifier:
    def __init__(self):
        self.bot = Bot(token=TELEGRAM_BOT_TOKEN)
        self.chat = TELEGRAM_CHAT_ID

    async def send(self, text):
        try:
            await self.bot.send_message(chat_id=self.chat, text=text, disable_web_page_preview=True)
            return True
        except RetryAfter as e:
            await asyncio.sleep(e.retry_after)
            return await self.send(text)
        except TelegramError as e:
            logger.error(f"TG error: {e}")
            return False

    async def startup(self):
        msg = (
            "ğŸš€ ×‘×•×˜ ××•×“×™×¢×™×Ÿ ×©×•×•×§×™× ×”×•×¤×¢×œ!\n\n"
            "ğŸ” ××¦×‘: ×¤×¢×™×œ\n"
            f"â° ×ª×“×™×¨×•×ª: ×›×œ {CHECK_INTERVAL} ×“×§×•×ª\n"
            "ğŸ“Š ××§×•×¨×•×ª: Polymarket, Kalshi, RSS\n"
            f'ğŸ¯ ×¡×£ ×”×ª×¨××”: {ALERT_THRESHOLD}+ × "×‘\n'
            "ğŸŒ ×©×¤×”: ×¢×‘×¨×™×ª\n"
            "ğŸ° ×›×•×œ×œ × ×™×ª×•×— ×”×©×¤×¢×” ×¢×œ ×”×™××•×¨×™×"
        )
        await self.send(msg)

    async def gap_alert(self, a):
        impact = analyze_impact(a["name"])
        msg = (
            f"ğŸ”” ×”×ª×¨××ª ×¤×¢×¨ â€” {a['name_he']}\n\n"
            f"ğŸ“Š ×©×•×§: {a['name_he']}\n"
            f"ğŸ·ï¸ ×§×˜×’×•×¨×™×”: {cat_he(a['cat'])}\n\n"
            f"Polymarket: {a['poly']}%\n"
            f"Kalshi: {a['kalshi']}%\n"
            f'ğŸ“ ×¤×¢×¨: {a["gap"]} × "×‘\n'
            f"ğŸ“ˆ ×›×™×•×•×Ÿ: {a['dir']}\n"
            f"{impact}\n\n"
            f"ğŸ”— Poly: {a['poly_url']}\n"
            f"ğŸ”— Kalshi: {a['kalshi_url']}"
        )
        await self.send(msg)

    async def move_alert(self, a):
        impact = analyze_impact(a["name"])
        msg = (
            f"âš¡ ×ª× ×•×¢×” ×’×“×•×œ×” â€” {a['name_he']}\n\n"
            f"ğŸ“Š {a['name_he']}\n"
            f"ğŸ·ï¸ ×§×˜×’×•×¨×™×”: {cat_he(a['cat'])}\n"
            f"××§×•×¨: {a['src']}\n\n"
            f"×œ×¤× ×™: {a['old']}% â†’ ×¢×›×©×™×•: {a['new']}%\n"
            f'ğŸ“ ×ª× ×•×¢×”: {a["delta"]} × "×‘\n'
            f"â±ï¸ ×˜×•×•×—: {a['tf']}\n"
            f"{impact}\n\n"
            f"ğŸ”— {a['url']}"
        )
        await self.send(msg)

    async def rss_alert(self, item):
        impact = analyze_impact(item["title"], item.get("summary", ""))
        msg = (
            f"ğŸ“° {item['feed']} â€” ×¢×“×›×•×Ÿ ×—×“×©\n\n"
            f"ğŸ“Œ {item['title_he']}\n\n"
            f"{item['summary_he'][:300]}\n"
            f"{impact}\n\n"
            f"ğŸ”— {item['link']}"
        )
        await self.send(msg)

    async def heartbeat(self, mc, fc):
        ts = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M UTC")
        msg = f"ğŸ’“ ×”×‘×•×˜ ×¤×¢×™×œ â€” {ts}\n×©×•×•×§×™× ×‘××¢×§×‘: {mc}\n×¤×™×“×™× ×‘××¢×§×‘: {fc}"
        await self.send(msg)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN SCAN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def scan(state, notifier):
    logger.info(f"â•â•â• Scan #{state.run_count + 1} â•â•â•")
    sent = 0

    try:
        async with aiohttp.ClientSession() as s:
            # Fetch
            poly = await fetch_polymarket(s)
            kalshi_data = await fetch_kalshi(s)

            # Gap detection
            gaps = match_and_find_gaps(poly, kalshi_data)
            for a in gaps[:5]:
                logger.info(f"ğŸ“Š GAP: {a['name']} â€” {a['gap']} bps")
                await notifier.gap_alert(a)
                sent += 1

            # Price tracking
            current = {}
            info = {}
            for m in poly + kalshi_data:
                current[m["id"]] = m["yes_price"]
                info[m["id"]] = {
                    "title": m["title"],
                    "title_he": m.get("title_he", m["title"]),
                    "category": m["category"],
                    "source": m["source"],
                    "url": m["url"],
                }

            # Big moves
            moves = find_big_moves(current, state.prices, {**state.info, **info})
            for a in moves[:5]:
                logger.info(f"âš¡ MOVE: {a['name']} â€” {a['delta']} bps")
                await notifier.move_alert(a)
                sent += 1

            state.prices = current
            state.info = info

            # RSS
            rss = await fetch_rss(s, since=state.get_last_run_dt())
            rss_sent = 0
            for item in rss:
                if item["guid"] in state.seen_guids:
                    continue
                if rss_sent >= 3:
                    break
                logger.info(f"ğŸ“° RSS: [{item['feed']}] {item['title']}")
                await notifier.rss_alert(item)
                state.seen_guids.append(item["guid"])
                sent += 1
                rss_sent += 1

            # Heartbeat every 12 runs (~24h)
            if state.run_count > 0 and state.run_count % 12 == 0:
                fc = sum(len(v) for v in RSS_FEEDS.values())
                await notifier.heartbeat(len(current), fc)

            state.run_count += 1
            state.last_run = datetime.now(timezone.utc).isoformat()
            state.save()

            logger.info(f"Done â€” {sent} alerts sent")

    except Exception as e:
        logger.exception(f"Scan error: {e}")
        try:
            await notifier.send(f"âš ï¸ ×©×’×™××”: {str(e)[:500]}")
        except Exception:
            pass


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENTRY POINT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def main():
    state = State()
    notifier = Notifier()

    logger.info("ğŸš€ Starting...")
    await notifier.startup()

    if "--once" in sys.argv:
        await scan(state, notifier)
        return

    scheduler = AsyncIOScheduler()
    scheduler.add_job(scan, IntervalTrigger(minutes=CHECK_INTERVAL),
                      args=[state, notifier], id="scan", max_instances=1,
                      misfire_grace_time=300)
    scheduler.start()

    await scan(state, notifier)

    logger.info(f"Scheduler active â€” every {CHECK_INTERVAL} min")
    stop = asyncio.Event()

    def handle_sig(sig, frame):
        logger.info("Shutting down...")
        stop.set()

    signal.signal(signal.SIGINT, handle_sig)
    signal.signal(signal.SIGTERM, handle_sig)

    await stop.wait()
    scheduler.shutdown(wait=False)


if __name__ == "__main__":
    asyncio.run(main())
